---
title: "Simulation Trials"
author: "Vignesh Balaji"
date: "2024-11-28"
output: html_document
---

# Simulate Power Trials

## Control Group

Set the time limit to be 48 weeks

1. Use the simulated baseline scores above.

2. Hypothesise a treatment decay factor for each transition ($\lambda_{21}$, $\lambda_{20}$, $\lambda_{10}$) for each item - assume that the decay factors are the same for each item to begin with, will have to consider non-uniform eventually. Might be useful to base off the median transition times based off the placebo data.

3. Then use this to simulate the \textbf{final} scores for each item - start with exponential function to give the times of transitions and then compare with the time limit of 48 weeks. So if the time to get to zero 

4. Transform the final scores for each individual using the a) NSAA framework b) PC1 at baseline framework c) Graded Response at baseline framework. This leaves us with a matrix / vector of total scores at the end of the trial.

## Treatment Group

Repeat all the steps as you did for the control group except the treatment effect will be a positive constant multiple of the original decays because it will elongate the time it takes for a patient to transition states.

We now have another vector of total scores at the end of the trial for the treatment group. 

## Power analysis

Perform a t-test on the end vectors for the treatment and control vectors. Repeat this a 1000 times to find a representative statistic of the power of the test. Then repeat over multiple treatment effects and plot a power curve! The one that can give a more powerful test for a given treatment effect is better. This is good because essentially you won't require as many participants which is the main bottleneck of current clinical trials because the disease is rare.

# Constant Effect

In this initial study we assume that the treatment effect is constant, that is to say that the treatment affects each item of the NSAA uniformly.

```{r}
library(readr)
library(tidyverse)
library(FactoMineR)
library(dplyr)
library(caret)
library(fastDummies)
library(factoextra)
library(mirt)
```

One hot encoding function
```{r}
encode <- function(df){
  df_fac <- lapply(df, as.factor)
  df_enc <- dummy_cols(
    df_fac,
    remove_first_dummy = F,
    remove_selected_columns = T
  )
  colnames(df_enc) <- gsub("^\\.data\\.", "", colnames(df_enc))
  colnames(df_enc) <- gsub("_", ".", colnames(df_enc))
  return(df_enc)
}
```

Simulate baseline scores of the whole cohort

```{r}
set.seed(42)

n_samples <- 260
n_items <- 17

latent_trait <- rnorm(n_samples, mean = 0, sd = 1) # use normal as hypothesised

simulate_scores <- function(latent_trait, n_items) {
  scores <- matrix(0, nrow = length(latent_trait), ncol = n_items)
  discrimination_params <- numeric(n_items)
  thresholds_list <- list()
  
  for (i in 1:n_items) {
    discrimination <- runif(1, 0.1, 2.0) # 'a' parameter
    thresholds <- sort(runif(2, -2, 2))   # 'b' parameters (thresholds)
    
    discrimination_params[i] <- discrimination
    thresholds_list[[i]] <- thresholds
    
    for (j in 1:length(latent_trait)) {
      theta <- latent_trait[j]
      # cumulative probabilities (logistic function)
      P_Y_geq_1 <- 1 / (1 + exp(-discrimination * (theta - thresholds[1])))
      P_Y_geq_2 <- 1 / (1 + exp(-discrimination * (theta - thresholds[2])))
      
      # state probabilities
      P0 <- 1 - P_Y_geq_1
      P1 <- P_Y_geq_1 - P_Y_geq_2
      P2 <- P_Y_geq_2
      
      probs <- c(P0, P1, P2)
      # sampling scores (n_samples x n_items)
      scores[j, i] <- sample(0:2, size = 1, prob = probs)
    }
  }
  
  # return dataframe, discrimination and threshold params for future ref
  list(
    scores = as.data.frame(scores), 
    discrimination_params = discrimination_params,
    thresholds = thresholds_list
  )
}

result <- simulate_scores(latent_trait, n_items)

item_scores <- result$scores
colnames(item_scores) <- paste0("Item", 1:n_items)

discrimination_params <- result$discrimination_params

thresholds <- result$thresholds

# find the most important items - this might come in handy later on when assigning treatment effect non-uniformly
barplot(discrimination_params, 
        main = "Discrimination Parameters (β1)", 
        xlab = "Items", 
        ylab = "Discrimination (β1)", 
        names.arg = paste0("Item", 1:n_items), 
        col = "blue")

# identify item difficulty based on thresholds
threshold_means <- sapply(thresholds, mean) 
difficulty_order <- order(threshold_means, decreasing = TRUE) 

data.frame(
  Item = paste0("Item", 1:n_items),
  Mean_Threshold = threshold_means
)[difficulty_order, ]

```

Split the baseline scores into a control and treatment group. In practise this would be done randomly but here we assign the first half to control. Then assign decay rates based off the difficulty of each item which is given by the threshold parameter used to generate the data.

```{r}
num_rows <- nrow(item_scores)
ctrl_grp_base <- item_scores[1:floor(num_rows / 2), ]
treat_grp_base <- item_scores[(floor(num_rows / 2) + 1):num_rows, ]

# assign decay factors based off the trial duration being a year
rate_21 <- 0.8
rate_10 <- 1.6
rate_20 <- 0.4

# Shift thresholds to make all values positive
shift_constant <- abs(min(unlist(thresholds))) + 1
shifted_thresholds <- lapply(thresholds, function(t) t + shift_constant)
threshold_means <- sapply(shifted_thresholds, mean)

# Generate decay rates proportional to shifted thresholds
decay_rates <- sapply(threshold_means, function(mean_thresh) {
  c(l_21 = mean_thresh * rate_21, 
    l_10 = mean_thresh * rate_10, 
    l_20 = mean_thresh * rate_20)
})
decay_rates <- t(decay_rates)

decay_rates_df <- data.frame(
  l_21 = decay_rates[, 1],
  l_10 = decay_rates[, 2],
  l_20 = decay_rates[, 3]
)

rownames(decay_rates_df) = paste0("Item", 1:n_items)
```


```{r}
# recursive survival function
simulate_decay <- function(current_state, l10, l21, l20, total_time, trial_time) {
  
  # state transitions
  if (current_state == 0) {
    return(0)
  } else if (current_state == 1) {
    # Transition from state 1 to 0
    t10 <- rexp(1, l10)
    total_time <- total_time + t10
    if (total_time <= trial_time) {
      return(simulate_decay(0, l10, l21, l20, total_time, trial_time))
    } else {
      return(1)
    }
  } else if (current_state == 2) {
    # transition from state 2 to either 1 or 0
    t21 <- rexp(1, l21)
    t20 <- rexp(1, l20)
    if (t20 <= t21) {
      total_time <- total_time + t20
      if (total_time <= trial_time) {
        return(simulate_decay(0, l10, l21, l20, total_time, trial_time))
      } else {
        return(2)
      }
    } else {
      total_time <- total_time + t21
      if (total_time <= trial_time) {
        return(simulate_decay(1, l10, l21, l20, total_time, trial_time))
      } else {
        return(2)
      }
    }
  }
}


# apply column wise to get ctrl_grp_end
ctrl_grp_end <- as.data.frame(apply(ctrl_grp_base, 2, function(col){
  sapply(col, 
         function(state) simulate_decay(state, decay_rates_df[col, 1],
                                        decay_rates_df[col, 2], 
                                        decay_rates_df[col, 3], 0, 1))
}))


# treatment effect
teff = 1.13

treat_grp_end <- as.data.frame(apply(treat_grp_base, 2, function(col){
  sapply(col, 
         function(state) simulate_decay(state, decay_rates_df[col, 1]/teff,
                                        decay_rates_df[col, 2]/teff, 
                                        decay_rates_df[col, 3]/teff, 0, 1))
}))
```

Transform using NSAA and getting PC1 from the one-hot encoded dataframes.
```{r}
scoring_method <- function(score, base=T, ctrl=T){
  if (score == "NSAA"){
    if (base){
      return(rowSums(ctrl_grp_base))
    }
    return(rowSums(ctrl_grp_end))
  }
  if (score == "PCA"){
    # hopefully this provides 52 columns
    combined_ctrl <- rbind(ctrl_grp_base, ctrl_grp_end)
    pca_ctrl <- prcomp(encode(combined_ctrl), scale.=T)
    pc1_ctrl <- pca_ctrl$rotation[, "PC1"]
    if (base){
      ctrl_base_pc1 <- as.matrix(encode(ctrl_grp_base)) %*%
        pc1_ctrl[colnames(encode(ctrl_grp_base))]
      return(ctrl_base_pc1)
    }
    ctrl_end_pc1 <- as.matrix(encode(ctrl_grp_end)) %*%
      pc1_ctrl[colnames(encode(ctrl_grp_end))]
    return(ctrl_end_pc1)
  }
  # add IRT here once you know how to extract the transformed scores
}

ctrl_nsaa_base <- scoring_method("NSAA")
ctrl_nsaa_end <- scoring_method("NSAA", base = F)
treat_nsaa_base <- scoring_method("NSAA", ctrl = F)
treat_nsaa_end <- scoring_method("NSAA", base = F, ctrl = F)

ctrl_pc1_base <- scoring_method("PCA")
ctrl_pc1_end <- scoring_method("PCA", base = F)
treat_pc1_base <- scoring_method("PCA", ctrl = F)
treat_pc1_end <- scoring_method("PCA", base = F, ctrl = F)
```

If function doesn't work
```{r}
# NSAA transformation - add up each item score individually
#ctrl_base_nsaa <- rowSums(ctrl_grp_base)
#treat_base_nsaa <- rowSums(treat_grp_base)
#ctrl_end_nsaa <- rowSums(ctrl_grp_end)
#treat_end_nsaa <- rowSums(treat_grp_end)


#combined_ctrl <- rbind(ctrl_grp_base, ctrl_grp_end)
#pca_ctrl <- prcomp(encode(combined_ctrl), scale.=T)
#pc1_ctrl <- pca_ctrl$rotation[, "PC1"]


#ctrl_base_pc1 <- as.matrix(encode(ctrl_grp_base)) %*% pc1_ctrl[colnames(encode(ctrl_grp_base))]
#treat_base_pc1 <- as.matrix(encode(treat_grp_base))
#ctrl_end_pc1 <- as.matrix(encode(ctrl_grp_end)) %*% 
  #pc1_ctrl[colnames(encode(ctrl_grp_end))]
#treat_end_pc1 <- as.matrix(encode(treat_grp_end)) %*%
  #pc1_ctrl[colnames(encode(treat_grp_end))]
```

An issue that I had above was that one particular item score wasn't visible at baseline but was visible at the end. As a quick fix, I combined control group datasets so that PC1 had 51 dimensions i.e. all the one-hot encoded columns. Check with Chris if this is the correct way of going about things? What would be a more robust method? Note that in a clinical trial we may not expect all scores to be realised.

Incorrect comparison of the NSAA and dynamic PC1.
```{r}
#n_pow <- n_samples / 2

# standard deviations for power calculation
#sd_ctrl_nsaa <- sd(ctrl_end_nsaa)
#sd_treat_nsaa <- sd(treat_end_nsaa)
#sd_ctrl_pc1 <- sd(ctrl_end_pc1)
#sd_treat_pc1 <- sd(treat_end_pc1)

#var_nsaa <- (sd_ctrl_nsaa^2 + sd_treat_nsaa^2) / 2
#var_pc1 <- (sd_ctrl_pc1^2 + sd_treat_pc1^2) / 2

# mean for power calculation
#mean_ctrl_nsaa <- mean(ctrl_end_nsaa)
#mean_treat_nsaa <- mean(treat_end_nsaa)
#mean_ctrl_pc1 <- mean(ctrl_end_pc1)
#mean_treat_pc1 <- mean(treat_end_pc1)

#d_nsaa <- mean_treat_nsaa - mean_ctrl_nsaa
#d_pc1 <- mean_treat_pc1 - mean_ctrl_pc1

#power.t.test(n = n_pow, delta = d_nsaa/sqrt(var_nsaa), sd = sqrt(var_nsaa),
             #sig.level = 0.05, type = "two.sample", alternative = "one.sided")

#power.t.test(n = n_pow, delta = d_pc1/sqrt(var_pc1), sd = sqrt(var_pc1),
             #sig.level = 0.05, type = "two.sample", alternative = "one.sided")
```

Findings: Now that I have performed the power calculations the PC1 derived from the control group (mainly from baseline) does better than the NSAA, as expected. This is because the standard deviation in the data is much smaller i.e. more homogeneous. But it seems very temperamental, in that treatment effects 1.1 - 1.4 lead to very small powers and then at 1.4 we have a big jump - is something going wrong in my calculation or is this the effect of the exponential distribution?

I am not doing the correct power calculation. You have to do 1000 simulations and fit a linear model control for baseline {NSAA, PC1} i.e. end = base*treatment + const. Then calculate power!