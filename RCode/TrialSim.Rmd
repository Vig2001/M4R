---
title: "Clinical Trial Simulations"
author: "Vignesh Balaji"
date: "2024-11-28"
output: html_document
---

# Simulate Power Trials

## Control Group

The duration of the trial is arbitrary and can be considered to be 1 year.

1. Simulate a large population (1000) of DMD NSAA item scores which can be used for fitting PCA and GRM to (see step 4 for more).

2. Hypothesise a treatment decay factor for each transition ($\lambda_{21}$, $\lambda_{20}$, $\lambda_{10}$) for each item - assume that the decay factors are the same for each item to begin with, will have to consider non-uniform eventually. Might be useful to base off the median transition times based off the placebo data.

3. Then use this to simulate the \textbf{final} scores for each item - start with exponential function to give the times of transitions and then compare with the time limit of 48 weeks. So if the time to get to zero 

4. Transform the final scores for each individual using the a) NSAA framework b) PC1 framework c) Graded Response framework. This leaves us with a vector of total scores at the end of the trial.

## Treatment Group

Repeat all the steps as you did for the control group except the treatment effect will be a positive constant multiple of the original decays because it will elongate the time it takes for a patient to transition states.

We now have another vector of total scores at the end of the trial for the treatment group. 

## Power analysis

Fit the end and baseline scores using a linear model and then find the significance of the Treatment Group and End Time coefficient. Repeat this a 1000 times to find a representative statistic of the power of the test. Then repeat over multiple treatment effects and plot a power curve. The framework that gives a more powerful test for a given treatment effect is better. This is good because essentially you won't require as many participants which is the main bottleneck of current clinical trials because the disease is rare.

# Constant Effect

In this initial study we assume that the treatment effect is constant, that is to say that the treatment affects each item of the NSAA uniformly.

```{r message=FALSE, warning=FALSE}
library(readr)
library(tidyverse)
library(FactoMineR)
library(dplyr)
library(caret)
library(fastDummies)
library(factoextra)
library(mirt)
library(reshape)
library(psych)
```

One hot encoding function
```{r}
encode <- function(df){
  df_fac <- lapply(df, as.factor)
  df_enc <- dummy_cols(
    df_fac,
    remove_first_dummy = F,
    remove_selected_columns = T
  )
  colnames(df_enc) <- gsub("^\\.data\\.", "", colnames(df_enc))
  colnames(df_enc) <- gsub("_", ".", colnames(df_enc))
  return(df_enc)
}
```

Simulate a large population sample
```{r}
set.seed(123)

n_samples <- 1000
n_items <- 17

latent_trait <- rnorm(n_samples, mean = 0, sd = 1) # higher score is better health

simulate_scores <- function(latent_trait, n_items) {
  scores <- matrix(0, nrow = length(latent_trait), ncol = n_items)
  discrimination_params <- numeric(n_items)
  thresholds_list <- list()
  
  for (i in 1:n_items) {
    # 'a' parameter - lognormal for positivity
    #discrimination <- runif(1, 0.1, 2.0)
    log_a <- rnorm(1, mean = 0, sd = 0.7)
    discrimination <- exp(log_a)
    # 'b' parameters (thresholds)
    # Note, we want ascending=T because 2 is a healthier score
    # So, only a healthy person would score 2
    #thresholds <- sort(runif(2, -2, 2))
    threshold1 <- rnorm(1, mean = 0, sd = 1)
    gap <- 0.5 + abs(rnorm(1, mean = 0, sd = 0.5)) # always geq to 0.5
    threshold2 <- threshold1 + gap # t2 >= t1
    thresholds <- c(threshold1, threshold2)
    
    discrimination_params[i] <- discrimination
    thresholds_list[[i]] <- thresholds
    
    for (j in 1:length(latent_trait)) {
      theta <- latent_trait[j]
      # cumulative probabilities (logistic function)
      P_Y_geq_1 <- 1 / (1 + exp(-discrimination * (theta - thresholds[1])))
      P_Y_geq_2 <- 1 / (1 + exp(-discrimination * (theta - thresholds[2])))
      
      # state probabilities
      P0 <- 1 - P_Y_geq_1
      P1 <- P_Y_geq_1 - P_Y_geq_2
      P2 <- P_Y_geq_2
      
      probs <- c(P0, P1, P2)
      # sampling scores (n_samples x n_items)
      scores[j, i] <- sample(0:2, size = 1, prob = probs)
    }
  }
  
  # return dataframe, discrimination and threshold params for future ref
  list(
    scores = as.data.frame(scores), 
    discrimination_params = discrimination_params,
    thresholds = thresholds_list
  )
}

result <- simulate_scores(latent_trait, n_items)

population_scores <- result$scores
colnames(population_scores) <- paste0("Item", 1:n_items)

discrimination_params <- result$discrimination_params

thresholds <- result$thresholds

# find the most important items
barplot(discrimination_params, 
        main = "Discrimination Parameters (β1)", 
        xlab = "Items", 
        ylab = "Discrimination (β1)", 
        names.arg = paste0("Item", 1:n_items), 
        col = "blue")

# identify item difficulty based on thresholds
#difficulty_order <- order(thresholds[1], decreasing = TRUE) 

#ordered_items_df <- data.frame(
                                #Item = paste0("Item", 1:n_items),
                                #Mean_Threshold = thresholds[1]
                              #)[difficulty_order, ]

```

Take a sample of the population to take part in a trial, and then split this into a control and treatment group. In practise this would be done randomly but here we assign the first half (of the sample) to control. Then assign decay rates based off the difficulty of each item which is given by the threshold parameter used to generate the data.

```{r}
thresholds_mat <- do.call(rbind, thresholds)
colnames(thresholds_mat) <- c("t1", "t2")

min_threshold <- min(thresholds_mat)
shift_value <- ifelse(min_threshold < 0, abs(min_threshold) + 0.2, 0)
thresholds_matrix <- thresholds_mat + shift_value

# Display the shift value
cat("Shift Value Applied to Thresholds:", shift_value, "\n\n")

# assign decay factors based off the trial duration being a year
rate_21 = 1.6
rate_10 = 0.8
rate_20 = 1 / ((1 / rate_21) + (1 / rate_10))

# Generate decay rates proportional to their difficulty
calculate_decay_rates <- function(thresh_matrix) {
  decay_rates <- data.frame(
    l_21 = thresh_matrix[, "t2"] * rate_21,
    l_10 = thresh_matrix[, "t2"] * rate_10,
    l_20= thresh_matrix[, "t2"] * rate_20
  )
  
  # Ensure no decay rate is below a minimal threshold (e.g., 0.1)
  decay_rates$l_21 <- pmax(decay_rates$l_21, 0.1)
  decay_rates$l_10 <- pmax(decay_rates$l_10, 0.1)
  decay_rates$l_20 <- pmax(decay_rates$l_20, 0.1)
  
  return(decay_rates)
}

decay_rates_df <- calculate_decay_rates(thresholds_matrix)
rownames(decay_rates_df) = paste0("Item", 1:n_items)
```


```{r}
# recursive survival function for simulating decay (markov chain function)
simulate_decay <- function(current_state, l10, l21, l20, total_time, trial_time) {
  
  # state transitions
  if (current_state == 0) {
    return(0)
  } else if (current_state == 1) {
    # Transition from state 1 to 0
    t10 <- rexp(1, l10)
    total_time <- total_time + t10
    if (total_time <= trial_time) {
      return(simulate_decay(0, l10, l21, l20, total_time, trial_time))
    } else {
      return(1)
    }
  } else if (current_state == 2) {
    # transition from state 2 to either 1 or 0
    t21 <- rexp(1, l21)
    t20 <- rexp(1, l20)
    if (t20 <= t21) {
      total_time <- total_time + t20
      if (total_time <= trial_time) {
        return(simulate_decay(0, l10, l21, l20, total_time, trial_time))
      } else {
        return(2)
      }
    } else {
      total_time <- total_time + t21
      if (total_time <= trial_time) {
        return(simulate_decay(1, l10, l21, l20, total_time, trial_time))
      } else {
        return(2)
      }
    }
  }
}
```

```{r}
# Add 30 anchors so that we get 0s, 1s and 2s in PCA and IRT
anchor_df <- data.frame(matrix(nrow = 30, ncol = 17))
colnames(anchor_df) <- paste0("Item", 1:n_items)
anchor_df[1:10, ] <- 0
anchor_df[11:20, ] <- 1
anchor_df[21:30, ] <- 2
```

Create PCA and IRT frameworks based off the whole population (RWD), which are globally defined variables.
```{r message=FALSE, warning=FALSE}
# PCA
resistant_df <- rbind(population_scores, anchor_df)
# one-hot encoded
pca_global <- prcomp(encode(resistant_df), scale. = T)
pc1_global <- pca_global$rotation[, "PC1"]
pcs_global <- pca_global$rotation[, c("PC1", "PC2")]

# polychoric
poly_corr <- polychoric(resistant_df)$rho
pca_polyglobal <- principal(poly_corr, nfactors = 2, rotate = "none")
pc1_polyglobal <- as.matrix(pca_polyglobal$loadings)[, "PC1"]

# IRT
invisible(capture.output({
  suppressWarnings(suppressMessages({
    grm_global <- mirt(resistant_df, 1, itemtype = "graded")
  }))
}))

pc_scores <- as.matrix(encode(population_scores)) %*% (pc1_global*-1)
pc_polyscores <- as.matrix(population_scores) %*% pc1_polyglobal
nsaa_scores <- rowSums(population_scores)
irt_global <- fscores(grm_global, response.pattern = population_scores, method="EAP")
cor.test(pc_scores, latent_trait, method = "spearman")
cor.test(nsaa_scores, latent_trait, method = "spearman")
cor.test(irt_global[ , 1], latent_trait, nethod = "spearman")
```

Below we define the bootstrap-compatible scoring function adapted from above - only change is that it takes in the datasets as input. Note that during boot iterations the sampled datasets will change!
```{r}
# cgb is ctrl_grp_base abbreviated
scoring_boot <- function(score, cgb, cge, tgb, tge, base = TRUE, ctrl = TRUE) {
  
  # get correct subset of data
  get_data <- function(base, ctrl) {
    if (ctrl) {
      if (base) return(cgb)
      else return(cge)
    } else {
      if (base) return(tgb)
      else return(tge)
    }
  }
  
  # NSAA helper
  do_nsaa <- function(base, ctrl) {
    subset_data <- get_data(base, ctrl)
    return(rowSums(subset_data))
  }
  
  # PCA helper
  do_pca <- function(base, ctrl) {
    
    subset_data <- encode(get_data(base, ctrl))
    #pc1_scores <- as.matrix(subset_data) %*% pc1_global[colnames(subset_data)]
    #pc_scores <- as.matrix(subset_data) %*% pcs_global[colnames(subset_data), ]
    #pc1_scores <- as.matrix(subset_data) %*% pc1_trial[colnames(subset_data)]
    #scores <- sqrt(rowSums(pc_scores^2))
    pc1_polyscores <- as.matrix(get_data(base, ctrl)) %*% pc1_polyglobal
    return(pc1_polyscores)
  }
  
  # IRT helper - this is wrong for IRT!
  do_irt <- function(base, ctrl) {

    subset_data <- get_data(base, ctrl)
    
    # transform subset_data
    irt_scores <- fscores(grm_global, response.pattern = subset_data,
                          method = "EAP")
    return(irt_scores[ , 1])
    
    
  }
  
  result <- switch(score,
    "NSAA" = do_nsaa(base, ctrl),
    "PCA"  = do_pca(base, ctrl),
    "IRT"  = do_irt(base, ctrl),
    "NA"
  )
  
  return(result)
}
```

Below we do not resimulate the baseline scores everytime, which means we can use the decay rates from decay_rates_df. Below we are finding empirical power estimates using bootstrap, for different teffs and method, then we can plot the power curves.
```{r}
n_pars <- 260 # number of trial participants
num_boots <- 1000 # number of bootstrap iters
teffs <- seq(1.1, 3, length.out = 10)
scoring_methods <- c("NSAA", "PCA", "IRT")

power_mat <- matrix(NA, nrow = length(teffs), ncol = length(scoring_methods))
rownames(power_mat) <- teffs
colnames(power_mat) <- scoring_methods

n_total <- length(teffs) * length(scoring_methods) * num_boots
pb_whole <- txtProgressBar(min = 0, max = n_total, style = 3)
counter <- 0

start_time <- Sys.time()

for (teff_idx in seq_along(teffs)) {
  teff <- teffs[teff_idx]
  
  for (method in scoring_methods) {
    
    # Timing stuff
    #cat("\nTeff:", teff, "   Method:", method, "\n")
    #pb <- txtProgressBar(min = 0, max = num_boots, style = 3)
    #start_time <- Sys.time()
    
    # Initialise num_boots vector to store power values
    power_iter <- numeric(num_boots)
    
    for (i in seq_len(num_boots)) {
      
      # 1a. Randomly sample patients from population_scores
      sample_indices <- sample(seq_len(n_pars), size = n_pars, 
                               replace = TRUE)
      sample_df <- population_scores[sample_indices, ]
      
      # 1b. Introduce Anchors for fitting PCA
      #fit_df <- rbind(sample_df, anchor_df)
      
      # 1c. Fit PCA to fit_df
      #pca_trial <- prcomp(encode(fit_df), scale. = T)
      #pc1_trial <- pca_trial$rotation[, "PC1"]
      
      # 2. Split into control and treatment (by halving the dataset)
      ctrlsample_base  <- sample_df[1:floor(n_pars / 2), ]
      treatsample_base <- sample_df[(floor(n_pars / 2) + 1):n_pars, ]
      
      # 3. Simulate Decays
      ctrlsample_end <- as.data.frame(apply(ctrlsample_base, 2, function(col) {
        sapply(col, function(state)
          simulate_decay(state, decay_rates_df[col, 1],
                                 decay_rates_df[col, 2],
                                 decay_rates_df[col, 3],
                                 0, 1))
      }))
      
      treatsample_end <- as.data.frame(apply(treatsample_base, 2, 
                                             function(col) {
        sapply(col, function(state)
          simulate_decay(state, decay_rates_df[col, 1] / teff,
                                 decay_rates_df[col, 2] / teff,
                                 decay_rates_df[col, 3] / teff,
                                 0, 1))
      }))
      
      # 4. Transform the scores using the scoring method
      ctrl_base  <- scoring_boot(method, ctrlsample_base,  ctrlsample_end,
                                 treatsample_base, treatsample_end)
      ctrl_end   <- scoring_boot(method, ctrlsample_base,  ctrlsample_end,
                                 treatsample_base, treatsample_end, 
                                 base = FALSE)
      treat_base <- scoring_boot(method, ctrlsample_base,  ctrlsample_end,
                                 treatsample_base, treatsample_end, 
                                 ctrl = FALSE)
      treat_end  <- scoring_boot(method, ctrlsample_base,  ctrlsample_end,
                                 treatsample_base, treatsample_end,
                                 base = FALSE, ctrl = FALSE)
      
      # 5. Build a dataframe for linear modelling
      boot_data <- data.frame(
        Score = c(ctrl_base, ctrl_end, treat_base, treat_end),
        Group = factor(c(rep(0, length(ctrl_base)),
                         rep(0, length(ctrl_end)),
                         rep(1, length(treat_base)),
                         rep(1, length(treat_end)))),
        Time  = factor(c(rep(0, length(ctrl_base)),
                         rep(1, length(ctrl_end)),
                         rep(0, length(treat_base)),
                         rep(1, length(treat_end))))
      )
      
      # 6. Fit linear model and gather p-value
      lm_boot <- lm(Score ~ Group * Time, data = boot_data)
      coefs <- summary(lm_boot)$coefficients
      p_val <- coefs["Group1:Time1", "Pr(>|t|)"]
      
      # 7. Record whether p < 0.05
      power_iter[i] <- as.numeric(p_val < 0.05)
      
      # Update progress bar
      #setTxtProgressBar(pb, i)\
      counter <- counter + 1
      setTxtProgressBar(pb_whole, counter)
      
      # 8. Overwrite the stopwatch on the next line
      current_time <- Sys.time()
      elapsed_time <- as.numeric(difftime(current_time, start_time, 
                                          units = "mins"))
      
      # Move back to the line below the progress bar: we use "\r" to overwrite
      # on that line, and no "\n" so it stays on that line
      cat(sprintf("\rElapsed time: %.2f minutes   ", elapsed_time))
      flush.console()
    }
    
    # Store power estimate
    power_mat[teff_idx, method] <- mean(power_iter)
    
    # Timing stuff
    #close(pb)
    
    #total_time <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
    #cat("Total elapsed time:", round(total_time, 2), "seconds\n")    
  }
}
```

```{r}
print(power_mat)
```

```{r message=FALSE, warning=FALSE}
# Add the treatment effect as a column for easier plotting
power_df <- as.data.frame(power_mat)
power_df$Teff <- as.numeric(rownames(power_df))

power_long <- melt(power_df, id.vars = "Teff", 
                   variable.name = "Method", 
                   value.name = "Power")
# power_long has columns: Teff, Variable and Value

#NSAA_PCA <- ggplot(
    #subset(power_long, variable %in% c("NSAA", "PCA")), 
    #aes(x = Teff, y = value, colour = variable)
  #) +
  #geom_line(size = 1) +
  #geom_point() +
  #geom_hline(yintercept = 0.8, colour = "red", linetype = "dashed") +
  #scale_y_continuous(limits = c(0, 1)) +
  #theme_minimal() +
  #theme(legend.position = c(0.05, 0.95),
        #legend.justification = c(0, 1)) +
  #labs(
    #title = "Power (NSAA & PCA) vs. Treatment Effect",
    #x = "Treatment Effect",
    #y = "Power"
  #)

#-----------------------------------------------------------
# Plotting all methods (NSAA, PCA, IRT)
#-----------------------------------------------------------
all_curves <- ggplot(power_long, aes(x = Teff, y = value, colour = variable)) +
  geom_line(size = 1) +
  geom_point() +
  geom_hline(yintercept = 0.8, colour = "red", linetype = "dashed") +
  scale_y_continuous(limits = c(0, 1)) +
  theme_minimal() +
  theme(legend.position = c(0.05, 0.95),
        legend.justification = c(0, 1)) +
  labs(
    title = "Power (All Methods) vs. Treatment Effect",
    x = "Treatment Effect",
    y = "Power"
  )


print(all_curves)
```

```{r}
ggsave(
  "../plots/power_plots/Methods_v3.png",
  all_curves,
  width = 8, 
  height = 6, 
  dpi = 300
)
```

Power curves varying sample size and keeping treatment effect constant.
```{r}
teff <- 2.5 # treatment effect
num_boots <- 1000 # number of bootstrap iters
n_trials <- seq(50, 250, length.out = 10)
scoring_methods <- c("NSAA", "PCA", "IRT")

power_matrix <- matrix(NA, nrow = length(n_trials), ncol = length(scoring_methods))
rownames(power_matrix) <- n_trials
colnames(power_matrix) <- scoring_methods

n_total <- length(n_trials) * length(scoring_methods) * num_boots
pb_whole <- txtProgressBar(min = 0, max = n_total, style = 3)
counter <- 0

start_time <- Sys.time()

for (n_idx in seq_along(n_trials)) {
  n_trial <- n_trials[n_idx]
  
  for (method in scoring_methods) {
    # Initialise num_boots vector to store power values
    power_iter <- numeric(num_boots)
    
    for (i in seq_len(num_boots)) {
      
      # 1a. Randomly sample patients from population_scores
      sample_indices <- sample(seq_len(n_trial), size = n_trial, 
                               replace = TRUE)
      sample_df <- population_scores[sample_indices, ]
      
      # 2. Split into control and treatment (by halving the dataset)
      ctrlsample_base  <- sample_df[1:floor(n_trial / 2), ]
      treatsample_base <- sample_df[(floor(n_trial / 2) + 1):n_trial, ]
      
      # 3. Simulate Decays
      ctrlsample_end <- as.data.frame(apply(ctrlsample_base, 2, function(col) {
        sapply(col, function(state)
          simulate_decay(state, decay_rates_df[col, 1],
                                 decay_rates_df[col, 2],
                                 decay_rates_df[col, 3],
                                 0, 1))
      }))
      
      treatsample_end <- as.data.frame(apply(treatsample_base, 2, 
                                             function(col) {
        sapply(col, function(state)
          simulate_decay(state, decay_rates_df[col, 1] / teff,
                                 decay_rates_df[col, 2] / teff,
                                 decay_rates_df[col, 3] / teff,
                                 0, 1))
      }))
      
      # 4. Transform the scores using the scoring method
      ctrl_base  <- scoring_boot(method, ctrlsample_base,  ctrlsample_end,
                                 treatsample_base, treatsample_end)
      ctrl_end   <- scoring_boot(method, ctrlsample_base,  ctrlsample_end,
                                 treatsample_base, treatsample_end, 
                                 base = FALSE)
      treat_base <- scoring_boot(method, ctrlsample_base,  ctrlsample_end,
                                 treatsample_base, treatsample_end, 
                                 ctrl = FALSE)
      treat_end  <- scoring_boot(method, ctrlsample_base,  ctrlsample_end,
                                 treatsample_base, treatsample_end,
                                 base = FALSE, ctrl = FALSE)
      
      # 5. Build a dataframe for linear modelling
      boot_data <- data.frame(
        Score = c(ctrl_base, ctrl_end, treat_base, treat_end),
        Group = factor(c(rep(0, length(ctrl_base)),
                         rep(0, length(ctrl_end)),
                         rep(1, length(treat_base)),
                         rep(1, length(treat_end)))),
        Time  = factor(c(rep(0, length(ctrl_base)),
                         rep(1, length(ctrl_end)),
                         rep(0, length(treat_base)),
                         rep(1, length(treat_end))))
      )
      
      # 6. Fit linear model and gather p-value
      lm_boot <- lm(Score ~ Group * Time, data = boot_data)
      coefs <- summary(lm_boot)$coefficients
      p_val <- coefs["Group1:Time1", "Pr(>|t|)"]
      
      # 7. Record whether p < 0.05
      power_iter[i] <- as.numeric(p_val < 0.05)

      counter <- counter + 1
      setTxtProgressBar(pb_whole, counter)
      
      # 8. Overwrite the stopwatch on the next line
      current_time <- Sys.time()
      elapsed_time <- as.numeric(difftime(current_time, start_time, 
                                          units = "mins"))
      
      # Move back to the line below the progress bar: we use "\r" to overwrite
      # on that line, and no "\n" so it stays on that line
      cat(sprintf("\rElapsed time: %.2f minutes   ", elapsed_time))
      flush.console()
    }
    
    # Store power estimate
    power_matrix[n_idx, method] <- mean(power_iter)
    
  }
}
```

Experimenting to see why PCA might be failing - main finding is that the loadings vary with time which hints at the fact that the item variability are not constant and thus is not a consistent measure of item discrimination / difficulty - causal abstraction as an extension.

```{r}
print(power_matrix)
```

```{r}
power_ss <- as.data.frame(power_matrix)
power_ss$SampleSize <- as.numeric(rownames(power_ss))

powerss_long <- melt(power_ss, id.vars = "SampleSize", 
                   variable.name = "Method", 
                   value.name = "Power")

sample_sizes <- ggplot(powerss_long, aes(x = SampleSize, y = value, colour = variable)) +
  geom_line(size = 1) +
  geom_point() +
  geom_hline(yintercept = 0.8, colour = "red", linetype = "dashed") +
  scale_y_continuous(limits = c(0, 1)) +
  theme_minimal() +
  theme(legend.position = c(0.05, 0.95),
        legend.justification = c(0, 1)) +
  labs(
    title = "Power (All Methods) vs. Treatment Effect",
    x = "Treatment Effect",
    y = "Power"
  )

print(sample_sizes)
```

```{r}
ggsave(
  "../plots/power_plots/sample_size.png",
  sample_sizes,
  width = 8, 
  height = 6, 
  dpi = 300
)
```