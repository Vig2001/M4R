---
title: "dmd-cspan-analysis"
author: "Vignesh Balaji"
date: "2024-03-25"
output:
  pdf_document: default
  html_document: default
---

```{r setup, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, include=FALSE}
library(pheatmap)
library(FactoMineR)
library(readr)
library(tidyverse)
library(dplyr)
library(ggtern)
library(factoextra)
library(nlme)
library(forecast)
library(lcmm)
library(lmeInfo)
library(boot)
library(xtable)
library(gridExtra)
library(ggrepel)
rs <- read_csv("Data/rs.csv")
```

## Data Cleaning

We perform some data analysis. Firstly, we only obtain data which belong to the study DMD-1005. We then filter so that we only get values of each NSAA component at baseline for each patient. The VISITNUM are in a nonsensical order because some patients are assigned a visit number of 99. This has to be investigated - hypothesis is that the patients which were assigned a VISITNUM of 99 did not make it past screening.

```{r, cache=TRUE}
# get only the relevant study
rs_1005 <- rs %>% filter(STUDYID == "DMD-1005")

# find all patients that participated in the study before screening
rs_1005_base <- rs_1005 %>% filter(VISITNUM == 0)

base_patients <- unique(rs_1005_base$USUBJID)

# find all patients that participated in the study before screening
rs_1005_base_check <- rs_1005 %>% filter(RSTEST == "NSAA1-Total", VISITNUM==0) %>% distinct(USUBJID, RSTEST, .keep_all = TRUE) 

# this should be the same length as base_patients
base_patients_check <- rs_1005_base_check$USUBJID

# find all patients that were screened
rs_1005_screened <- rs_1005 %>% filter(VISITNUM == 1)

screened_patients <- unique(rs_1005_screened$USUBJID)

# check if all base patients began screening
print(length(intersect(base_patients, screened_patients)))
# yes they did

# find all patients that weren't removed from screening
rs_1005_begin <- rs_1005 %>% filter(VISITNUM == 4)

begin_patients <- unique(rs_1005_begin$USUBJID)

# check if all base patients began the study
print(length(intersect(base_patients, begin_patients)))
# three patients failed screening

failed_screen <- setdiff(base_patients, begin_patients)

# find all patients that dropped out
rs_1005_end <- rs_1005 %>% filter(VISITNUM == 99)

dropout_patients = unique(rs_1005_end$USUBJID)

# investigate how many of the patients that failed screening were given VN = 99
print(length(intersect(failed_screen, dropout_patients)))
# none of them
# this is good
# All patients that dropped out therefore will have VISITNUM >= 5
```

Idea: Iterate through the dropped out patients. Find their second largest visit number (x), as their largest will be 99, then change VISITNUM = 99 to VISITNUM = x+1

```{r, cache=TRUE}
for(patient_id in dropout_patients) {
  # Find the second largest VISITNUM for the current patient excluding 99
  second_largest_visitnum <- rs_1005 %>%
    filter(USUBJID == patient_id, VISITNUM != 99) %>%
    arrange(desc(VISITNUM)) %>%
    slice(1) %>%
    pull(VISITNUM)
  
  # Check if there is a valid second largest VISITNUM
  if(length(second_largest_visitnum) > 0) {
    # Update the VISITNUM from 99 to second largest + 1
    rs_1005 <- rs_1005 %>%
      mutate(VISITNUM = if_else(USUBJID == patient_id & VISITNUM == 99, second_largest_visitnum + 1, VISITNUM))
  }
}

# This is just so that the visit numbers are from 0-9
mapping <- setNames(c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), c(0, 1, 4, 5, 6, 7, 8, 9, 10, 11))

rs_1005$VISITNUM <- mapping[as.character(rs_1005$VISITNUM)]
```

Let's now extract the matrix and create a function which generalises this process for any VISITNUM.

```{r,cache=TRUE}
# first we have to investigate all the factors that make up RSTEST column
RSTEST_classes <- unique(rs_1005$RSTEST)

# remove NSAA1_Total and NSAA1_Total Linearized
totals_to_remove <- which(RSTEST_classes %in% c("NSAA1-Total", "NSAA1-Total Linearized"))

RSTEST_classes <- RSTEST_classes[-totals_to_remove]

# also remove any grades, timing and velocity results
keywords_to_remove <- c("Velocity", "Time", "Grade")
misc_to_remove <- grep(paste0(paste(keywords_to_remove, collapse = "|"), collapse = "|"), RSTEST_classes)

RSTEST_classes <- RSTEST_classes[-misc_to_remove]

RSTEST_list <- as.list(RSTEST_classes)

rs_1005_restricted <- rs_1005_base %>% filter(RSTEST %in% RSTEST_list)

# remove duplicated rows
rs_1005_data <- rs_1005_restricted %>% distinct(USUBJID, RSTEST, 
                                                .keep_all = TRUE)

# use pivot wider to create the desired df
rs_1005_pivot <- rs_1005_data %>% select(USUBJID, RSTEST, RSORRES) %>%
  pivot_wider(id_cols = USUBJID,
              names_from = RSTEST,
              values_from = RSORRES)
```

```{r,cache=TRUE}
# convert to matrix removing the first column as that is subject ID
X0 <- as.matrix(rs_1005_pivot[2:18])
# fill na with mean value of the column
X0 <- apply(X0, 2, as.numeric)
column_means <- colMeans(X0, na.rm = TRUE) 
X0[is.na(X0)] <- column_means[rep(1:ncol(X0), each = nrow(X0))]

# define a function which does this for any VISITNUM
create_matrix <- function(df, VN){
  df_new <- df %>% filter(VISITNUM == VN)
  RSTEST_classes <- unique(df$RSTEST)
  totals_to_remove <- which(RSTEST_classes %in% c("NSAA1-Total", "NSAA1-Total Linearized"))
  RSTEST_classes <- RSTEST_classes[-totals_to_remove]
  keywords_to_remove <- c("Velocity", "Time", "Grade")
  misc_to_remove <- grep(paste0(paste(keywords_to_remove, collapse = "|"), collapse = "|"), RSTEST_classes)

  RSTEST_classes <- RSTEST_classes[-misc_to_remove]

  RSTEST_list <- as.list(RSTEST_classes)

  df_restricted <- df_new %>% filter(RSTEST %in% RSTEST_list)
  df_data <- df_restricted %>% distinct(USUBJID, RSTEST, 
                                                .keep_all = TRUE)
  df_pivot <- df_data %>% select(USUBJID, RSTEST, RSORRES) %>%
  pivot_wider(id_cols = USUBJID,
              names_from = RSTEST,
              values_from = RSORRES)
  
  X <- as.matrix(df_pivot[2:18])
  X <- apply(X, 2, as.numeric)
  column_means <- colMeans(X, na.rm = TRUE) 
  X[is.na(X)] <- column_means[rep(1:ncol(X), each = nrow(X))]
  return(X)
}
```

## Visualising Trajectories
```{r}
rs_1005_distinct <- rs_1005 %>%
  filter(RSTEST == "NSAA1-Total") %>%
  distinct(USUBJID, VISITNUM, RSORRES, .keep_all = TRUE)

rs_plot <- rs_1005_distinct %>%
  filter(RSTEST == "NSAA1-Total") %>%
  group_by(USUBJID) %>%
  mutate(last_visit = VISITNUM == max(VISITNUM)) %>%
  ungroup()

dropoutplot <- ggplot(data = rs_plot, aes(x = VISITNUM, y = as.numeric(RSORRES), group = USUBJID)) +
  geom_line(color = "gray", size = 1) +
  geom_point(aes(color = ifelse(last_visit, "dropout", "active")), size = 2, shape = 21, fill = ifelse(rs_plot$last_visit, "red", "blue")) +
  scale_color_manual(values = c("active" = "blue", "dropout" = "red"),
                     labels = c("active" = "Active", "dropout" = "Dropout")) +
  scale_x_continuous(breaks = c(0, 3, 6, 9)) +
  labs(title = "Patient Trajectories with Dropout",
       x = "Visit Number",
       y = "NSAA Total Score",
       color = "Patient Status") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

Create table of means and s.d. for each visit
```{r}
rs_1005_distinct$RSORRES <- as.numeric(rs_1005_distinct$RSORRES)

# Summarize data: calculate mean, sd, and count for each VISITNUM
summary_stats <- rs_1005_distinct %>%
  filter(RSTEST == "NSAA1-Total") %>%
  group_by(VISITNUM) %>%
  summarise(
    mean = mean(RSORRES, na.rm = TRUE),
    sd = sd(RSORRES, na.rm = TRUE),
    n = n(),
    .groups = 'drop'  # Drop grouping after summarization
  ) %>%
  mutate(label = sprintf("%.2f (%.2f), n = %d", mean, sd, n))  # Format mean, sd, and n into a single string

print(summary_stats)
```

Visualise the different stages of disease.
```{r, warning=FALSE}
# choose three patients who are at different stages
# DMD-1005/508-80853 - decreasing
# DMD-1005/018-87850 - increasing
# DMD-1005/018-87881 - constant

specific_patients <- rs_plot %>% filter(USUBJID %in% c("DMD-1005/018-87850", "DMD-1005/018-87881"))

diff_stagesplot <- ggplot(data = specific_patients, aes(x = VISITNUM, y = as.numeric(RSORRES), group = USUBJID)) +
  geom_line(color = "gray", size = 1) +
  geom_point(aes(color = ifelse(last_visit, "dropout", "active")), size = 2, shape = 21, fill = ifelse(specific_patients$last_visit, "red", "blue")) +
  scale_color_manual(values = c("active" = "blue", "dropout" = "red"),
                     labels = c("active" = "Active", "dropout" = "Dropout")) +
  scale_x_continuous(breaks = c(0, 3, 6, 9)) +
  labs(title = "Specific Patient Trajectories",
       x = "Visit Number",
       y = "NSAA Total Score",
       color = "Patient Status") +
  theme_minimal() +
  theme(legend.position = "bottom")
```


## Baseline & Temporal MVar Relationship

Should we use scale = TRUE or scale = FALSE? scale = TRUE will standardize each feature or variable before we perform PCA. This can be particularly useful in certain cases, for instance if one variable takes larger values it will exhibit larger variance by nature, this then translates to it potentially falsely dominating PC1. To combat this, we standardize the data so that we have a “global” scale for all variables, i.e. the analysis will be invariant to the measurement scale.

In this setting, we have that each component of the NSAA takes a value of 0, 1 or 2 and so are on the same scale. However, one task may be inherently easier to do than the other, and so has an abnormally higher frequency of 2s. So, to answer the question scale = TRUE or FALSE, we have to investigate the proportion of 0s, 1s and 2s for each component (outputted as barycentric coordinates), as well as the variance-covariance matrix to formulate a robust conclusion.

```{r, cache=TRUE}
# create correlation heatmap at baseline
cov_matrix <- cor(X0)
corrheatmap <- pheatmap(cov_matrix, scale='none')
```

```{r}
colnames(X0) <- gsub("NSAA1-", "", colnames(X0))

# Create data frame with updated column names
feature_stats <- data.frame(
  feature = colnames(X0),
  prop_mean = apply(X0, 2, mean) / 2,
  prop_mean2 = (apply(X0, 2, mean) / 2)**2,
  variance = apply(X0, 2, var)
)

# Define the range of means to plot the Bernoulli relationship over
mean_range <- seq(min(feature_stats$prop_mean), max(feature_stats$prop_mean), length.out = length(feature_stats$prop_mean))

# find the best fitting curve by plotting variance against p(1-p) because they are proportional

mvar_lm <- lm(variance ~ prop_mean + prop_mean2, data = feature_stats)
summary(mvar_lm)
# optimal dispersion parameter is roughly 2.
```

```{r}
bernoulli_variance <- 2 * mean_range * (1 - mean_range)  # Bernoulli variance across the range

# Create scatter plot with actual data and Bernoulli relationship
mvarplot <- ggplot(feature_stats, aes(x = prop_mean, y = variance, label = feature)) +
  geom_point() +  # Actual data points
  geom_label_repel(nudge_y = 0.05, box.padding = 0.35, point.padding = 0.5, segment.color = 'grey50') +
  geom_line(aes(x = mean_range, y = bernoulli_variance, color = "Bin(2, p) Mean-Variance Relationship"), linetype = "dashed") + # Expected Bernoulli relationship
  labs(x = "Mean Proportion", y = "Variance", title = "Mean-Variance Relationship of each NSAA Item") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(title = NULL))

print(mvarplot)
```

```{r, cache=TRUE}
rs_1005_counts <- rs_1005 %>%
  filter(RSTEST %in% RSTEST_list) %>%
  group_by(RSTEST, VISITNUM, RSORRES) %>%
  summarise(count = n(), .groups = 'drop')

rs_1005_totals <- rs_1005_counts %>%
  group_by(RSTEST, VISITNUM) %>%
  summarise(total = sum(count), .groups = 'drop')

rs_1005_proportions <- rs_1005_counts %>%
  left_join(rs_1005_totals, by = c("RSTEST", "VISITNUM")) %>%
  mutate(proportion = count / total) %>%
  select(RSTEST, VISITNUM, RSORRES, proportion)

rs_1005_proportions <- rs_1005_proportions %>%
  pivot_wider(names_from = RSORRES, values_from = proportion, values_fill = list(proportion = 0))
```

We want to plot the trajectories of each component on a barycentric plot

```{r, warning=FALSE, cache=TRUE}
plot_ternary <- function(test_component){
rs_1005_bplot <- rs_1005_proportions %>% filter(RSTEST == test_component)

plot <- ggtern(data = rs_1005_bplot, aes(x = `0`, y = `1`, z = `2`)) +
  geom_point(aes(color = factor(VISITNUM)), size = 3) + geom_jitter() +
  geom_path(arrow = arrow(type = "open", length = unit(0.15, "inches")),
            color="blue", size = 0.5) +
  theme_bw() + ggtitle(test_component) + 
  labs(x = "0", y = "1", z = "2", color = "Visit Number") +
  theme(
    plot.title = element_text(hjust = 0.5),  # Center the title
    legend.position = "right"
  )
return(plot)
}
```


## PCA

```{r, warning=FALSE}
# apply PCA
pca_result <- prcomp(X0, scale. = TRUE) 

S <- pca_result$sdev

# scree plot
plot(1:length(S), S^2, type = "b",
     xlab = "Principal Component", ylab = "Eigenvalue",
     main = "Scree Plot")

# loadings
loadings_matrix <- pca_result$rotation
# the biplot shows that hop left and right are really correlated with one another
# might be best to use one leg
biplot(pca_result, cex = 0.7, arrow.len=0.001, scale = 0.8)
print(loadings_matrix[,1:2])

plot(X0%*%loadings_matrix[, 1])
```

```{r, warning=FALSE}
# nicer plots
pca_facto <- PCA(X0, graph = FALSE, scale.unit=FALSE)

#if (!dir.exists("plots")) {
  #dir.create("plots")
#}

# Generate and save scree plot
my_screeplot <- fviz_screeplot(pca_facto, addlabels = TRUE, ylim = c(0, 100), 
               main = "Scree Plot")

# Generate and save biplot
my_biplot <- fviz_pca_biplot(pca_facto, geom = "point", col.var = "contrib", 
                gradient.cols = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf"), 
                repel = TRUE, arrow.len = 0.001, 
                label = "var", cex = 0.5, pointsize = 2, labelsize = 5, 
                repel.box.padding = 0.2, repel.segment.size = 0.3)

# loadings matrix
loadings_matrix_facto <- pca_facto$var$coord

print(loadings_matrix_facto[, 1:2]) # loadings are a factor of e off prcomp - ask chris about this
```

```{r, warning=FALSE}
# plot trajectory of the weight for each component within PC1 and PC2 against time points

num_visits =  6 # number of visits until dropout affects results
num_features = length(RSTEST_list)
PC1s <- matrix(NA, nrow=num_features, ncol=num_visits)
PC2s <- matrix(NA, nrow=num_features, ncol=num_visits)
num_points <- rep.int(0, times=num_visits)
#sort(unique(rs_1005$VISITNUM)
for (v in 0:5){
  X <- create_matrix(rs_1005, v)
  pca <- PCA(X, graph = FALSE) 
  PC1 <- pca$var$coord[ ,1]
  PC2 <- pca$var$coord[ ,2]
  PC1s[, v+1] <- PC1
  PC2s[, v+1] <- PC2
  num_points[v+1] <- nrow(X)
}

png("plots/PC1s_scaled.png", width = 5000, height = 2000, res = 300)
par(mar = c(5, 4, 4, 2) + 0.1, oma = c(0, 0, 0, 18), cex.lab=1.6, cex.main=2.4)
# PC1s
plot(1:num_visits, PC1s[1, ], type = "l", col = 1, xlab = "Visit", ylab = "PC1", xlim = c(1, num_visits), ylim = range(PC1s), main = "PC1 Weights (Scaled)")
for (row in 2:num_features) {
  lines(1:num_visits, PC1s[row, ], col = row)
}
legend("right", inset = -0.3, legend = RSTEST_list, col = 1:num_features, lty = 1, xpd = NA, cex = 1)
dev.off()

png("plots/PC2s_scaled.png", width = 5000, height = 2000, res = 300)
par(mar = c(5, 4, 4, 2) + 0.1, oma = c(0, 0, 0, 18), cex.lab=1.6, cex.main=2.4)
plot(1:num_visits, PC2s[1, ], type = "l", col = 1, xlab = "Visit", ylab = "PC2", xlim = c(1, num_visits), ylim = range(PC2s), main = "PC2 Weights (Scaled)")

for (row in 2:num_features) {
  lines(1:num_visits, PC2s[row, ], col = row)
}
legend("right", inset = -0.3, legend = RSTEST_list, col = 1:num_features, lty = 1, xpd = NA, cex = 1)
dev.off()

# You're looking at how each variable loads onto the first two PCs and seeing how that loading varies with time. 
# NSAA claims that we have a 1d measure of disease severity
```
The distribution of principal component weights of each component at a specific visit number has a mean of 0.7, but it is clear to see some outliers and the variance in the distribution increase with visit. Therefore, it is hard to justify adding each component equally to give a total NSAA score, even after taking into account that some components display different variability. A potential approach to combat this is to set the total score to be a linear combination of the components, where each scale factor is the PC1 weight of each item.

```{r}
# plot trajectory of the weight for each component within PC1 and PC2 against time points

num_visits =  6 #length(unique(rs_1005$VISITNUM))
num_features = length(RSTEST_list)
PC1s_f <- matrix(NA, nrow=num_features, ncol=num_visits)
PC2s_f <- matrix(NA, nrow=num_features, ncol=num_visits)
num_points <- rep.int(0, times=num_visits)
for (v in 0:5){
  X <- create_matrix(rs_1005, v)
  pca <- PCA(X, graph = FALSE, scale.unit = FALSE) 
  PC1 <- pca$var$coord[ ,1]
  PC2 <- pca$var$coord[ ,2]
  PC1s_f[, v+1] <- PC1
  PC2s_f[, v+1] <- PC2
  num_points[v+1] <- nrow(X)
}

png("plots/PC1s_unscaled.png", width = 5000, height = 2000, res = 300)
par(mar = c(5, 4, 4, 2) + 0.1, oma = c(0, 0, 0, 18), cex.lab=1.6, cex.main=2.4)
plot(1:num_visits, PC1s_f[1, ], type = "l", col = 1, xlab = "Visit", ylab = "PC1", xlim = c(1, num_visits), ylim = range(PC1s_f), main = "PC1 Weights (Raw)")
for (row in 2:num_features) {
  lines(1:num_visits, PC1s_f[row, ], col = row)
}
legend("right", inset = -0.3, legend = RSTEST_list, col = 1:num_features, lty = 1, xpd = NA, cex = 1)
dev.off()

png("plots/PC2s_unscaled.png", width = 5000, height = 2000, res = 300)
par(mar = c(5, 4, 4, 2) + 0.1, oma = c(0, 0, 0, 18), cex.lab=1.6, cex.main=2.4)
plot(1:num_visits, PC2s_f[1, ], type = "l", col = 1, xlab = "Visit", ylab = "PC2", xlim = c(1, num_visits), ylim = range(PC2s_f), main = "PC1 Weights (Raw)")
for (row in 2:num_features) {
  lines(1:num_visits, PC2s_f[row, ], col = row)
}
legend("right", inset = -0.3, legend = RSTEST_list, col = 1:num_features, lty = 1, xpd = NA, cex = 1)
dev.off()
```
When we don't apply scaling to our principal component analysis, we see a very sparse distribution in the weights of both principal components through time. This further damages the rationale behind adding up the components equally to give a total NSAA score out of 34.

## GLS Diagnostics

```{r}
new_trajectories <- matrix(0, 131, 6) 

# Initialize a vector to keep track of all patient IDs
patient_ids <- seq_len(131)

# Initialize a logical vector to track patients
active_patients <- rep(TRUE, 131)

for (i in 0:5) {
  X <- create_matrix(rs_1005, i)
  
  # Determine the number of active patients for this visit
  num_active <- nrow(X)
  
  # Update the active patients status based on the matrix size
  if (num_active < length(active_patients)) {
    # Assume the missing patients are the ones who dropped out
    active_patients[(num_active + 1):length(active_patients)] <- FALSE
  }
  
  active_indices <- which(active_patients)
  
  result_vector <- X %*% loadings_matrix[ ,1]
  
  new_trajectories[active_indices, i + 1] <- result_vector
}

trajectory_df <- as.data.frame(new_trajectories)

trajectory_df$PatientID <- seq_len(nrow(trajectory_df))

trajectory_long_df <- pivot_longer(trajectory_df, 
                                   cols = -PatientID, 
                                   names_to = "Visit", 
                                   values_to = "Trajectory",
                                   names_prefix = "V")

trajectory_long_df$Visit <- as.numeric(gsub("V", "", trajectory_long_df$Visit))

trajectory_long_df <- trajectory_long_df %>%
  group_by(PatientID) %>%
  mutate(Dropout = ifelse(Trajectory == 0, "Dropout", "Active"),
         Trajectory = ifelse(Dropout == "Dropout" & lag(Dropout, default = "Active") == "Active", 0, Trajectory),
         Trajectory = ifelse(lag(Dropout, default = "Active") == "Dropout", NA, Trajectory))

# Plotting using ggplot2
plot <- ggplot(trajectory_long_df, aes(x = Visit, y = Trajectory, group = PatientID)) +
  geom_line(color = "grey", size = 1) + 
  geom_point(aes(color = Dropout), size = 2, shape = 21, fill=NA) +
  scale_color_manual(values = c("Active" = "blue", "Dropout" = "red")) +
  labs(x = "Visit Number", y = "Scaled NSAA Total", title = "Patient Trajectories Over Visits") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(plot)
```
We calculate the correlation parameter using gls on the total NSAA score.
```{r}
# get dataset of only NSAA
rs_1005_nsaa <- rs_1005 %>% filter(RSTEST == "NSAA1-Total")
# change the visit column so that it records integer times
# baseline and screening are given a time stamp of 0 weeks
# every other time stamp is (n-1) * 12 weeks
rs_1005_nsaa <- rs_1005_nsaa %>%
  mutate(VISIT = case_when(
    VISITNUM %in% c(0, 1) ~ 0,
    VISITNUM >= 2 & VISITNUM <= 9 ~ (VISITNUM - 1) * 12,
    TRUE ~ NA_real_
  ))

# now we can define the gls model
# remove duplicated rows
rs_1005_nsaa <- rs_1005_nsaa %>% distinct(USUBJID, VISIT, 
                                                .keep_all = TRUE)

# make RSORRES numeric
rs_1005_nsaa$RSORRES <- as.numeric(rs_1005_nsaa$RSORRES)

# output summary
nsaa_total_gls <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2), 
                        data = rs_1005_nsaa, 
                        correlation = corAR1(form = ~ 1 | USUBJID))

# the correlation under visit is just the correlation between visit coefficient and the intercept.
summary(nsaa_total_gls)
```

Perform bootstrapping
```{r}
# manually perform bootstrap to get the autocorrelation parameter
patient_ids <- unique(rs_1005_nsaa$USUBJID)
boot_phis <- rep(0, 1000)

# define function that gets the phi parameter in a boot sample
gls_phi <- function(patient_ids){
  # choose only the patients that appear in the bootstrap sample
  boot_data = rs_1005_nsaa %>% filter(USUBJID %in% unique(patient_ids))
  # model grouped by each patient so different patients aren't correlated
  gls_model <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2), 
                        data = boot_data, 
                        correlation = corAR1(form = ~ 1 | USUBJID))
  # output the phi parameter
  phi <- extract_varcomp(gls_model)[1]
  return(as.numeric(phi))
}

for (reps in 1:1000){
  # sample with replacement from patient_ids
  boot_indices <- sample(1:131, size = 131, replace = TRUE)
  boot_patients <- patient_ids[boot_indices]
  boot_phi <- gls_phi(boot_patients)
  boot_phis[reps] <- boot_phi
}

# 1000 bootstrap samples
# histogram of results
phi_data <- data.frame(Phi = boot_phis)
phi_bar <- mean(phi_data$Phi)
sd_phi <- sd(phi_data$Phi)

phi_histogram <- ggplot(phi_data, aes(x = Phi)) + 
  geom_histogram(aes(y = ..density..), bins = 30, fill = "lightblue", color = "black") +
  stat_function(fun = dnorm, args = list(mean = phi_bar, sd = sd_phi),
                aes(color = "Gaussian Density"), size = 1) +
  scale_color_manual(name = "Gaussian Parameters", 
                     values = "black", 
                     labels = paste("Mean =", round(phi_bar, 4), "SD =", round(sd_phi, 4))) +
  ggtitle("Histogram of Phi Parameters") +
  xlab("Phi Parameter") +
  ylab("Density") +
  theme_minimal() +
  theme(legend.position = "top")

print(phi_histogram)
mean_phi <- mean(boot_phis)
print(mean_phi)
```

```{r}
gls_coeffs <- function(patient_ids) {
  boot_data <- rs_1005_nsaa %>% filter(USUBJID %in% unique(patient_ids))
  
  gls_model <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2), 
                   data = boot_data, 
                   correlation = corAR1(form = ~ 1 | USUBJID))
  
  coeffs <- coef(gls_model)
  return(coeffs)
}

boot_coeffs <- data.frame(Intercept = double(1000), VISIT = double(1000), VISIT_squared = double(1000))

for (reps in 1:1000) {
  boot_indices <- sample(patient_ids, size = length(patient_ids), replace = TRUE)
  
  coeffs <- gls_coeffs(boot_indices)
  
  boot_coeffs$Intercept[reps] <- coeffs['(Intercept)']
  boot_coeffs$VISIT[reps] <- coeffs['VISIT']
  boot_coeffs$VISIT_squared[reps] <- coeffs['I(VISIT^2)']
}

mean_intercept <- mean(boot_coeffs$Intercept)
sd_intercept <- sd(boot_coeffs$Intercept)
mean_visit <- mean(boot_coeffs$VISIT)
sd_visit <- sd(boot_coeffs$VISIT)
mean_visit_squared <- mean(boot_coeffs$VISIT_squared)
sd_visit_squared <- sd(boot_coeffs$VISIT_squared)


p0 <- ggplot(boot_coeffs, aes(x = Intercept)) + 
  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "red", color = "black") +
  stat_function(fun = dnorm, args = list(mean = mean_intercept, sd = sd_intercept), 
                aes(color = "Gaussian Density"), size = 1) +
  scale_color_manual(name = "Gaussian Parameters", 
                     values = "black", 
                     labels = sprintf("Mean = %.4f, SD = %.4f", mean_intercept, sd_intercept)) +
  ggtitle("Histogram of Intercept Estimates") +
  xlab("Intercept") +
  ylab("Density") +
  theme_minimal() +
  scale_y_continuous(labels = function(x) x * 100) +
  theme(legend.position = "top")

p1 <- ggplot(boot_coeffs, aes(x = VISIT)) + 
  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "blue", color = "black") +
  stat_function(fun = dnorm, args = list(mean = mean_visit, sd = sd_visit), 
                aes(color = "Gaussian Density"), size = 1) +
  scale_color_manual(name = "Gaussian Parameters", 
                     values = "black", 
                     labels = sprintf("Mean = %.4f, SD = %.4f", mean_visit, sd_visit)) +
  ggtitle("Histogram of Linear Coefficient Estimates") +
  xlab("Coefficient of VISIT") +
  ylab("Density") +
  theme_minimal() +
  theme(legend.position = "top")


p2 <- ggplot(boot_coeffs, aes(x = VISIT_squared)) + 
  geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "green", color = "black") +
  stat_function(fun = dnorm, args = list(mean = mean_visit_squared, sd = sd_visit_squared), 
                aes(color = "Gaussian Density"), size = 1) +
  scale_color_manual(name = "Gaussian Parameters", 
                     values = "black", 
                     labels = sprintf("Mean = %.5f, SD = %.4f", mean_visit_squared, sd_visit_squared)) +
  ggtitle("Histogram of Quadratic Coefficient Estimates") +
  xlab("Coefficient of VISIT Squared") +
  ylab("Density") +
  theme_minimal() +
  scale_y_continuous(labels = function(x) x / 100) +
  theme(legend.position = "top")


print(p0)
print(p1)
print(p2)

mean_intercept <- mean(boot_coeffs$Intercept)
mean_visit <- mean(boot_coeffs$VISIT)
mean_visit_squared <- mean(boot_coeffs$VISIT_squared)
print(paste("Mean Intercept:", mean_intercept))
print(paste("Mean Coefficient for VISIT:", mean_visit))
print(paste("Mean Coefficient for VISIT Squared:", mean_visit_squared))
```



```{r}
# plot the residuals as a whole
png("plots/res_vs_fitted_whole.png", width=3000, height=2000, res=300)
plot(nsaa_total_gls, resid(., type="p")~fitted(.), abline=0)
dev.off()
# plot the residuals for every visit
# check if there is clustering within the residuals
# we assume no grouping when using this model - maybe if there is clustering in the residuals we might need to use a different model
png("plots/residualsbyvisit.png", width=3000, height=4000, res=300)
par(cex=1.5)
plot(nsaa_total_gls, resid(., type="p")~fitted(.)|VISIT, abline=0)
dev.off()
# qqnorm to see if standardized residuals are standard normal
png("plots/qqnorm_gls_res.png", width=3000, height=1000, res=300)
qqnorm(nsaa_total_gls, abline=c(0,1))
dev.off()

# check autocorrelation within residuals for a random subject
residuals_data <- data.frame(residuals = residuals(nsaa_total_gls),
                             USUBJID = rs_1005_nsaa$USUBJID)
random_subject <- sample(unique(residuals_data$USUBJID), 1)
subject_residuals <- subset(residuals_data,
                            USUBJID == random_subject)$residuals

# plot PACF
# We cannot derive a correlation structure based off the PACF
# The PACF for each individual changes
# Moroever the correlation values themselves at certain lag points aren't significant enough to draw a conclusion from
png("plots/subject_residuals.png", width=3000, height=2000, res=300)
pacf(subject_residuals, main=paste("PACF for Subject", random_subject))
dev.off()
# fit the model without ar1 correlation - then plot the residuals and check its autocorrelation.
nsaa_uncorrelated_gls <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2),
                             data = rs_1005_nsaa)
residuals_nsaa <- residuals(nsaa_uncorrelated_gls)
# Can clearly observe a lag 1 correlation structure which is encapsulated by the AR1 in our original model.
png("plots/uncorrelated_residuals.png", width=3000, height=2000, res=300)
pacf(residuals_nsaa, lag.max=10, main="PACF of Uncorrelated Residuals")
dev.off()
```

The summary indicates an AR(1) correlation structure in our GLS model, with a high phi estimate of 0.9639783, suggesting strong correlation between the nsaa scores of a single patient between two visits (i.e. 12 weeks apart). This is as expected because someone who scores highest at visit 1 (arbitrary) is likely to score highest at visit 2, regardless of the stage of disease. Correlation is independent of the trend.

Let's interpret the standardized residuals analysis. Upon analysing both standardized residuals vs the fitted values graphs with and without the VISIT grouping structure, we can conclude that there is no obvious grouping of subjects through time i.e. not enough evidence to suggest a clustering of trajectories. The fitted values in this sense are the NSAA total scores - so one can visualise time essentially moving in the reverse direction to the fitted values such that the last vertical line of points represents the baseline measurement. We can make this deduction by examining the plots for each VISIT - by comparing the sets of plots with the "overarching" plot we can see that each VISIT make up the vertical "pillars" observed. The standardized residuals follow a standard normal distribution for each visit (as illustrated by the qqnorm plot) which backs our model specification and once again undermines the clustering claim. For an obvious grouping structure to be present we would've expected certain residual points being bunched up together at each visit point, but this is not completely obvious from the plots and would've drastically affected the qqnorm plot. 

The deliverable - if we have a treatment effect which causes an improvement in the NSAA by 'x' amount how many patients do we need in the trial?

How does clustering relate to this? If we have clusters then the model will change and the treatment effect may be different between clusters - so we performed clustering as a sense check to see if there is a meaningful way to separate the patients - idea for further work.

We will investigate how the size of the treatment effect affects its significance in the model
```{r}
treatment_effect_fn <- function(data, num_bootstraps) {
  p_values <- numeric(num_bootstraps)
  patient_ids <- unique(data$USUBJID)  # Get unique patient IDs once

  for (i in 1:num_bootstraps) {
    boot_patients <- sample(patient_ids, size = length(patient_ids), replace = TRUE)
    
    sampled_data <- data %>% filter(USUBJID %in% boot_patients)
    
    selected_patients <- sample(boot_patients, size = length(boot_patients) / 2)
    
    sampled_data <- sampled_data %>%
      mutate(TREAT = ifelse(USUBJID %in% selected_patients, 1, 0),
             RSORRES = ifelse(TREAT == 1, RSORRES + 5, RSORRES))
    
    gls_model <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2) + TREAT, data = sampled_data, 
                     correlation = corAR1(form = ~ 1 | USUBJID))
    summary_gls <- summary(gls_model)
    
    coefficients <- summary_gls$tTable[, "Value"]
    standard_errors <- summary_gls$tTable[, "Std.Error"]
    z_scores <- coefficients / standard_errors
    p_values[i] <- 2 * pnorm(abs(z_scores["TREAT"]), lower.tail = FALSE)
  }
  
  return(p_values)
}

set.seed(123)
bootstrap_p_values <- treatment_effect_fn(rs_1005_nsaa, num_bootstraps = 1000)
sig_values <- sum(bootstrap_p_values < 0.05) / length(bootstrap_p_values)

boot_teff <- ggplot(data.frame(PValue = bootstrap_p_values), aes(x = PValue)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Treatment Effect p-values from Bootstrap", x = "p-value", y = "Frequency")

print(sig_values)
```
Here we print out the proportion of significant p-values (i.e. p-values under 0.05). This is an unbiased estimator for the statistical power of the hypothesis test we are conducting (the hypothesis test is essentially is there enough evidence to say that the mean trajectory of an untreated and treated patient is the same), which is a test to investigate whether the treatment effect has an observable impact on the mean NSAA trajectories. When the proportion of significant p-values is close to 1 this illustrates we have a strong statistical test, which translates to the probability that the test correctly rejects the null hypothesis (in this scenario this is the hypothesis that the treatment effect has no observable impact). Therefore, if we have a proportion for \verb|sig_values| close to 1 then if there is indeed a treatment effect the test is high likely to detect it. Conversely, if it is closer to 0 then the test is unlikely to detect the treatment effect. We can see that a treatment effect of 5 gives us our desired result (a proportion > 0.8) for a sample size of 131 patients. Now we must simulate how the sample size affects the power.

```{r}
rho <- mean_phi # 0.96
sigma <- 5 # modify std

N <- 100 # Number of patients
visits <- 1:10 # Visits from 1 to 10
treatment_effect <- 5

# Using rough numbers
intercept <- 10
visit_coef <- 2.5
visit_squared_coef <- -0.5


# Ensure first half of patients are treated - for t test purposes
treatment_assignments <- ifelse(1:N <= N/2, 1, 0)

# Simulate AR(1) residuals for one patient (10 visits)
simulate_ar1_residuals <- function(n, rho, sigma) {
  e <- rnorm(n, mean = 0, sd = sigma)
  residuals <- vector(length = n)
  residuals[1] <- e[1]
  
  for (i in 2:n) {
    residuals[i] <- rho * residuals[i - 1] + e[i]
  }
  
  return(residuals)
}

simulate_patient_data <- function(patient_id, treat, visits, rho, sigma) {
  n <- length(visits)
  ar1_residuals <- simulate_ar1_residuals(n, rho, sigma)
  
  rsorres <- intercept + 
             visit_coef * visits + 
             visit_squared_coef * visits^2 + 
             treat * treatment_effect +
             ar1_residuals
  
  # Ensure correct range and integer type
  rsorres <- round(pmin(pmax(rsorres, 0), 34))
  
  data.frame(
    USUBJID = rep(patient_id, n),
    VISIT = visits,
    TREAT = rep(treat, n),
    RSORRES = rsorres
  )
}


patient_data <- lapply(1:N, function(i) {
  treat <- sample(c(0,1), 1)
  simulate_patient_data(i, treat, visits, rho, sigma)
})


simulated_data <- bind_rows(patient_data)

gls_model <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2) + TREAT, 
                      data = simulated_data, 
                      correlation = corAR1(form = ~ 1 | USUBJID))
summary_gls <- summary(gls_model)
print(summary_gls)
```

```{r}
bootstrap_analysis <- function(data, num_bootstraps) {
  p_values <- numeric(num_bootstraps)
  
  for (i in 1:num_bootstraps) {
    boot_patient_ids <- sample(unique(data$USUBJID), size = length(unique(data$USUBJID)), replace = TRUE)
    boot_data <- data %>% filter(USUBJID %in% boot_patient_ids)
    
    gls_model <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2) + TREAT, data = boot_data, 
                     correlation = corAR1(form = ~ 1 | USUBJID))
    summary_gls <- summary(gls_model)
    
    coefficients <- summary_gls$tTable[, "Value"]
    standard_errors <- summary_gls$tTable[, "Std.Error"]
    z_scores <- coefficients / standard_errors
    p_values[i] <- 2 * pnorm(abs(z_scores["TREAT"]), lower.tail = FALSE)
  }
  
  return(p_values)
}

set.seed(123)

set.seed(123)
bootstrap_p_values <- bootstrap_analysis(simulated_data, num_bootstraps = 1000)
sig_values <- sum(bootstrap_p_values < 0.05) / length(bootstrap_p_values)

boot_sim_ps <- ggplot(data.frame(PValue = bootstrap_p_values), aes(x = PValue)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of Treatment Effect p-values from Bootstrap", x = "p-value", y = "Frequency")

print(sig_values)
```

## Statistical Power Analysis

```{r}
# Using rough numbers
generate_data <- function(N, treatment_effect){

visits <- 1:10
intercept <- 10
visit_coef <- 2.5
visit_squared_coef <- -0.5

treatment_assignments <- ifelse(1:N <= N/2, 1, 0)

simulate_ar1_residuals <- function(n, rho, sigma) {
  e <- rnorm(n, mean = 0, sd = sigma)
  residuals <- vector(length = n)
  residuals[1] <- e[1]
  
  for (i in 2:n) {
    residuals[i] <- rho * residuals[i - 1] + e[i]
  }
  
  return(residuals)
}

simulate_patient_data <- function(patient_id, treat, visits, rho, sigma) {
  n <- length(visits)
  ar1_residuals <- simulate_ar1_residuals(n, rho, sigma)
  
  rsorres <- intercept + 
             visit_coef * visits + 
             visit_squared_coef * visits^2 + 
             treat * treatment_effect +
             ar1_residuals
  
  rsorres <- round(pmin(pmax(rsorres, 0), 34))
  
  data.frame(
    USUBJID = rep(patient_id, n),
    VISIT = visits,
    TREAT = rep(treat, n),
    RSORRES = rsorres
  )
}

patient_data <- lapply(1:N, function(i) {
  treat <- sample(c(0,1), 1)
  simulate_patient_data(i, treat, visits, rho, sigma)
})
simulated_data <- bind_rows(patient_data)
return(simulated_data)
}

power_calc <- function(N, treatment_effect){
  set.seed(123)
data <- generate_data(N, treatment_effect)
bootstrap_p_values <- bootstrap_analysis(data, num_bootstraps = 500)
sig_values <- sum(bootstrap_p_values < 0.05) / length(bootstrap_p_values)
return(sig_values)
}

treatment_effects <- seq(0, 5, length.out = 5)
sample_sizes <- c(50, 100, 150, 200)
results <- data.frame()


set.seed(123)
for (size in sample_sizes) {
  for (effect in treatment_effects) {
    power <- power_calc(size, effect)
    results <- rbind(results, data.frame(SampleSize = size, EffectSize = effect, Power = power))
  }
}

power_plots <- ggplot(results, aes(x = EffectSize, y = Power, color = as.factor(SampleSize))) +
  geom_line() +
  geom_hline(yintercept = 0.8, linetype = "dashed", color = "red")  +
  labs(title = "Power Curve for Different Sample Sizes",
       x = "Treatment Effect Size",
       y = "Power",
       color = "Sample Size") +
  theme_minimal()

print(power_plots)
```