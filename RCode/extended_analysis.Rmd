---
title: "extended_analysis"
author: "Vignesh Balaji"
date: "2024-10-31"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(tidyverse)
library(FactoMineR)
library(dplyr)
library(caret)
library(factoextra)
```

# Extended PCA

Here we attempt to illustrate a proof of concept for a new one-dimensional scoring system based off the PC1 of the chosen individual items. The idea is that we shall inspect the barycentric plots of each item and remove items which play very little impact in differentiating between patients, as exemplified through their temporal mean-variance relationship.

## Baseline Analysis

Read in baseline data
```{r}
Visit0 <- read.csv("Data/baseline_scores.csv")
```

From visualising the plots, we can identify problematic variables as:

\item Get to Sitting
\item Rise from Floor
\item Stand
\item Stand on Heels
\item Walk

There are also some features that we should keep an eye on:

\item Hop Left and Right Legs
\item Stand up from Chair
\item Stand on one Leg - Left & Right

We should also consider whether we need asymmetric items - experiment with removing left leg activities and assume that right leg encodes enough information by itself.

```{r}
drop_names <- c("NSAA1.Get.to.Sitting", "Rise.From.Floor", "NSAA1.Stand",
                "NSAA1.Stand.on.Heels", "NSAA1.Walk")

Visit0_reg <- Visit0[, !(colnames(Visit0) %in% drop_names)]

# create df without USUBJID
rownames(Visit0_reg) <- Visit0_reg$USUBJID
Visit0_df <- Visit0_reg[, -1]
Visit0_df[] <- lapply(Visit0_df, as.factor)

# one hot encoding
dummy <- dummyVars("~ .", data = Visit0_df)
Visit0_enc <- data.frame(predict(dummy, newdata = Visit0_df))

Visit0_mat <- as.matrix(Visit0_enc)
```

# PCA and MCA

Perform PCA on the encoded variables after scaling each column
```{r}
# scaling
prop_ones <- colMeans(Visit0_mat == 1)

scale_factors <- sqrt(prop_ones * (1 - prop_ones))

Visit0_mat_scaled <- sweep(Visit0_mat, 2, scale_factors, "/")

pca_base <- PCA(Visit0_mat_scaled, graph = FALSE, scale.unit=FALSE)

base_screeplot <- fviz_screeplot(pca_base, addlabels = TRUE, ylim = c(0, 100), 
               main = "Scree Plot")

print(base_screeplot)
```
Let's perform MCA on the original regularised data. MCA is used as an alternative for PCA when using categorical data - from my understanding it is equivalent to PCA on one-hot encoded variables except it uses the chi-square distance rather than the euclidean distance.
```{r}
mca_base <- MCA(Visit0_df, graph = FALSE)

# Summary of MCA
summary(mca_base)

# Scree plot of eigenvalues to understand variance explained by dimensions
fviz_screeplot(mca_base, addlabels = TRUE, ylim = c(0, 50))
```

It is clear that to capture a large proportion of the variance we need to incorporate a multi-dimensional scoring system i.e. we have to instruct doctors to perform some form of matrix multiplication at each visit! However, this lacks interpretability and may not generalise well to new data.

## Temporal Analysis

Here we will pull in the datasets of the remaining visits which aren't severely affected by dropout (visit 1 - visit 5). Then experiment with the time-wise PCA approach:

\begin{enumerate}
\item Stick to 1 PC
\item Stick to a fixed number of PCs
\item Vary the dimensions with time, so that we explain most of the cross-sectional variance at any given time
\end{enumerate}

The cost of the last choice is that we might lose interpretability and we don't really understand the progression from visit to visit.

ChatGPT made the following code to dynamically get dataframes.

```{r}
# Directory containing the CSV files
data_dir <- "Data"

# Get a list of all CSV files in the directory
file_names <- list.files(path = data_dir, pattern = "\\.csv$", full.names = TRUE)

# Loop through each file and read it into a separate data frame
for (file in file_names) {
  # Create a name based on the file name, removing the ".csv" extension
  object_name <- gsub("\\.csv$", "", basename(file))
  
  # Read the CSV file and assign it to the dynamically created name
  assign(object_name, read.csv(file))
}
```


One-hot encode and convert to matrices for all dataframes.
```{r}
clean_df <- function(df){
  df_reg <- df[, !(colnames(df) %in% drop_names)]
  rownames(df_reg) <- df_reg$USUBJID
  df_reg <- df_reg[, -(1:2)]
  return(df_reg)
}

encode <- function(df){
  df_fac <- lapply(df, as.factor)
  onehot <- dummyVars("~ .", data = df_fac, fullRank = FALSE)
  X_enc <- data.frame(predict(onehot, newdata = df_fac))
  # Preserve original row names
  rownames(X_enc) <- rownames(df)
  return(X_enc)
}

Visit1_df <- as.data.frame(clean_df(Visit1))
Visit1_mat <- as.matrix(encode(Visit1_df))
Visit2_df <- as.data.frame(clean_df(Visit2))
Visit2_mat <- as.matrix(clean_df(Visit2))
Visit3_df <- as.data.frame(clean_df(Visit3))
Visit3_mat <- as.matrix(encode(Visit3_df))
Visit4_df <- as.data.frame(clean_df(Visit4))
Visit4_mat <- as.matrix(encode(Visit4_df))
Visit5_df <- as.data.frame(clean_df(Visit5))
Visit5_mat <- as.matrix(encode(Visit5_df))
```

Now we perform dynamic time-wise PCA.

1. We apply PCA to each visit
2. We obtain ndim - the number of dimensions where the explained_variance is first above 75%
3. Get the loadings matrix with ndim principal components
4. Transform the score
5. Sum the scores to get the new total
6. Create a new dataset with these scores
```{r, warnings=FALSE}
# transformed scores for visit function
transformed_totals <- function(Visit_mat){
  # initial PCA to find number of dimensions
  pca_res <- PCA(Visit_mat, graph = F, scale.unit = T)
  eig_df <- as.data.frame(get_eigenvalue(pca_res))
  cum_var <- eig_df$cumulative.variance.percent
  # select a cutoff - experiment
  ndim <- which.min(abs(cum_var - 90))
  if (cum_var[ndim] < 75){
    ndim <- ndim + 1
  }
  # set ncp in new PCA - experiment with it
  pca_new <- PCA(Visit_mat, graph = F, scale.unit = T, ncp = ndim)
  loadings <- pca_new$var$cor
  # get transformed scores
  scores <- Visit_mat %*% loadings
  # sum to get the new NSAA Total
  totals <- as.data.frame(rowSums(scores))
  rownames(totals) <- rownames(scores)
  return(totals)
}

totals_V0 <- transformed_totals(Visit0_mat)
totals_V1 <- transformed_totals(Visit1_mat)
totals_V2 <- transformed_totals(Visit2_mat)
totals_V3 <- transformed_totals(Visit3_mat)
totals_V4 <- transformed_totals(Visit4_mat)
totals_V5 <- transformed_totals(Visit5_mat)
```

Here we merge the datasets together to get a new dataset
```{r}
list_of_totals <- list(totals_V0, totals_V1, totals_V2, totals_V3, totals_V4, totals_V5)

merged_totals <- totals_V0

# merge sequentially by rowname to get new longitudinal dataset
for (i in 2:length(list_of_totals)) {
  merged_totals <- merge(
    merged_totals, 
    list_of_totals[[i]], 
    by = "row.names", 
    all = TRUE
  )
  
  # Set rownames and remove the added Row.names column
  rownames(merged_totals) <- merged_totals$Row.names
  merged_totals$Row.names <- NULL
}

# Replace NA with 0
merged_totals[is.na(merged_totals)] <- 0

# Rename columns if necessary (e.g., to V0, V1, V2, etc.)
colnames(merged_totals) <- paste0("V", 0:(ncol(merged_totals) - 1))

# Display the result
print(merged_totals)
```

```{r}
# Transpose the dataframe so that each row is plotted as a line
library(ggplot2)
library(reshape2)

# Convert the dataframe to long format
merged_totals$PatientID <- rownames(merged_totals)  # Add PatientID column
merged_totals_long <- melt(
  merged_totals, 
  id.vars = "PatientID", 
  variable.name = "TimePoint", 
  value.name = "Value"
)

# Convert TimePoint to numeric for plotting
merged_totals_long$TimePoint <- as.numeric(gsub("V", "", merged_totals_long$TimePoint))

# Create a ggplot
ggplot(merged_totals_long, aes(x = TimePoint, y = Value, color = PatientID, group = PatientID)) +
  geom_line() +
  labs(
    title = "Plot of Each Row as a Line",
    x = "Visit Number",
    y = "Values"
  ) +
  theme_minimal() + 
  theme(legend.position = "none")
```