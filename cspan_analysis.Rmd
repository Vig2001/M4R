---
title: "dmd-cspan-analysis"
author: "Chris Hallsworth"
date: "2023-08-01"
output:
  pdf_document: default
  html_document: default
---

```{r setup, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, include=FALSE}
library(FactoMineR)
library(readr)
library(tidyverse)
library(dplyr)
library(ggtern)
library(factoextra)
library(nlme)
library(forecast)
library(lcmm)
library(lmeInfo)
library(boot)
rs <- read_csv("rs.csv")
```

Chris' snippet of code

```{r}
#rs_clean <- rs %>% filter(STUDYID == "DMD-1005",
                          #RSTEST == "NSAA1-Total",
                          #VISITNUM <= 15) 
#rs_clean$nsaa <- as.numeric(rs_clean$RSORRES)

#ggplot(data = rs_clean,
       #mapping  = aes(x = VISITNUM ,
                      #y = nsaa,
                      #group = USUBJID)) + geom_line()
```

Now we perform some data analysis. Firstly, we only obtain data which belong to the study DMD-1005. We then filter so that we only get values of each NSAA component at baseline for each patient. The VISITNUM are in a nonsensical order because some patients are assigned a visit number of 99. This has to be investigated - hypothesis is that the patients which were assigned a VISITNUM of 99 did not make it past screening.

```{r}
# get only the relevant study
rs_1005 <- rs %>% filter(STUDYID == "DMD-1005")

# find all patients that participated in the study before screening
rs_1005_base <- rs_1005 %>% filter(VISITNUM == 0)

base_patients <- unique(rs_1005_base$USUBJID)

# find all patients that participated in the study before screening
rs_1005_base_check <- rs_1005 %>% filter(RSTEST == "NSAA1-Total", VISITNUM==0) %>% distinct(USUBJID, RSTEST, .keep_all = TRUE) 

# this should be the same length as base_patients
base_patients_check <- rs_1005_base_check$USUBJID

# find all patients that were screened
rs_1005_screened <- rs_1005 %>% filter(VISITNUM == 1)

screened_patients <- unique(rs_1005_screened$USUBJID)

# check if all base patients began screening
print(length(intersect(base_patients, screened_patients)))
# yes they did

# find all patients that weren't removed from screening
rs_1005_begin <- rs_1005 %>% filter(VISITNUM == 4)

begin_patients <- unique(rs_1005_begin$USUBJID)

# check if all base patients began the study
print(length(intersect(base_patients, begin_patients)))
# three patients failed screening

failed_screen <- setdiff(base_patients, begin_patients)

# find all patients that dropped out
rs_1005_end <- rs_1005 %>% filter(VISITNUM == 99)

dropout_patients = unique(rs_1005_end$USUBJID)

# investigate how many of the patients that failed screening were given VN = 99
print(length(intersect(failed_screen, dropout_patients)))
# none of them
# this is good
# All patients that dropped out therefore will have VISITNUM >= 5
```

Idea: Iterate through the dropped out patients. Find their second largest visit number (x), as their largest will be 99, then change VISITNUM = 99 to VISITNUM = x+1

```{r}
for(patient_id in dropout_patients) {
  # Find the second largest VISITNUM for the current patient excluding 99
  second_largest_visitnum <- rs_1005 %>%
    filter(USUBJID == patient_id, VISITNUM != 99) %>%
    arrange(desc(VISITNUM)) %>%
    slice(1) %>%
    pull(VISITNUM)
  
  # Check if there is a valid second largest VISITNUM
  if(length(second_largest_visitnum) > 0) {
    # Update the VISITNUM from 99 to second largest + 1
    rs_1005 <- rs_1005 %>%
      mutate(VISITNUM = if_else(USUBJID == patient_id & VISITNUM == 99, second_largest_visitnum + 1, VISITNUM))
  }
}

# This is just so that the visit numbers are from 0-9
mapping <- setNames(c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), c(0, 1, 4, 5, 6, 7, 8, 9, 10, 11))

rs_1005$VISITNUM <- mapping[as.character(rs_1005$VISITNUM)]
```

Now we get the matrix that we input into PCA from this

```{r}
# first we have to investigate all the factors that make up RSTEST column

RSTEST_classes <- unique(rs_1005$RSTEST)

# remove NSAA1_Total and NSAA1_Total Linearized
totals_to_remove <- which(RSTEST_classes %in% c("NSAA1-Total", "NSAA1-Total Linearized"))

RSTEST_classes <- RSTEST_classes[-totals_to_remove]

# also remove any grades, timing and velocity results
keywords_to_remove <- c("Velocity", "Time", "Grade")
misc_to_remove <- grep(paste0(paste(keywords_to_remove, collapse = "|"), collapse = "|"), RSTEST_classes)

RSTEST_classes <- RSTEST_classes[-misc_to_remove]

RSTEST_list <- as.list(RSTEST_classes)

rs_1005_restricted <- rs_1005_base %>% filter(RSTEST %in% RSTEST_list)

# remove duplicated rows
rs_1005_data <- rs_1005_restricted %>% distinct(USUBJID, RSTEST, 
                                                .keep_all = TRUE)

# use pivot wider to create the desired df
rs_1005_pivot <- rs_1005_data %>% select(USUBJID, RSTEST, RSORRES) %>%
  pivot_wider(id_cols = USUBJID,
              names_from = RSTEST,
              values_from = RSORRES)
```

Let's now extract the matrix and create a function which generalises this process for any VISITNUM.

```{r}
# convert to matrix removing the first column as that is subject ID
X0 <- as.matrix(rs_1005_pivot[2:18])
# fill na with mean value of the column
X0 <- apply(X0, 2, as.numeric)
column_means <- colMeans(X0, na.rm = TRUE) 
X0[is.na(X0)] <- column_means[rep(1:ncol(X0), each = nrow(X0))]

# define a function which does this for any VISITNUM
create_matrix <- function(df, VN){
  df_new <- df %>% filter(VISITNUM == VN)
  RSTEST_classes <- unique(df$RSTEST)
  totals_to_remove <- which(RSTEST_classes %in% c("NSAA1-Total", "NSAA1-Total Linearized"))
  RSTEST_classes <- RSTEST_classes[-totals_to_remove]
  keywords_to_remove <- c("Velocity", "Time", "Grade")
  misc_to_remove <- grep(paste0(paste(keywords_to_remove, collapse = "|"), collapse = "|"), RSTEST_classes)

  RSTEST_classes <- RSTEST_classes[-misc_to_remove]

  RSTEST_list <- as.list(RSTEST_classes)

  df_restricted <- df_new %>% filter(RSTEST %in% RSTEST_list)
  df_data <- df_restricted %>% distinct(USUBJID, RSTEST, 
                                                .keep_all = TRUE)
  df_pivot <- df_data %>% select(USUBJID, RSTEST, RSORRES) %>%
  pivot_wider(id_cols = USUBJID,
              names_from = RSTEST,
              values_from = RSORRES)
  
  X <- as.matrix(df_pivot[2:18])
  X <- apply(X, 2, as.numeric)
  column_means <- colMeans(X, na.rm = TRUE) 
  X[is.na(X)] <- column_means[rep(1:ncol(X), each = nrow(X))]
  return(X)
}
```

Now we perform PCA, after constructing our matrix.

Should we use scale = TRUE or scale = FALSE? scale = TRUE will standardize each feature or variable before we perform PCA. This can be particularly useful in certain cases, for instance if one variable takes larger values it will exhibit larger variance by nature, this then translates to it potentially falsely dominating PC1. To combat this, we standardize the data so that we have a “global” scale for all variables, i.e. the analysis will be invariant to the measurement scale.

In this setting, we have that each component of the NSAA takes a value of 0, 1 or 2 and so are on the same scale. However, one task may be inherently easier to do than the other, and so has an abnormally higher frequency of 2s. So, to answer the question scale = TRUE or FALSE, we have to investigate the proportion of 0s, 1s and 2s for each component (outputted as barycentric coordinates), as well as the variance-covariance matrix to formulate a robust conclusion.

```{r}
# print out the variances from the covariance matrix
print(diag(cov(X0))) # <- some evidence for scale=TRUE (variances of each component do be different).
```

```{r}
# Calculate the counts for each RSORRES value within each RSTEST and VISITNUM group
rs_1005_counts <- rs_1005 %>%
  filter(RSTEST %in% RSTEST_list) %>%
  group_by(RSTEST, VISITNUM, RSORRES) %>%
  summarise(count = n(), .groups = 'drop')

# Calculate the total counts for each RSTEST and VISITNUM group
rs_1005_totals <- rs_1005_counts %>%
  group_by(RSTEST, VISITNUM) %>%
  summarise(total = sum(count), .groups = 'drop')

# Join the counts with the totals to calculate proportions
rs_1005_proportions <- rs_1005_counts %>%
  left_join(rs_1005_totals, by = c("RSTEST", "VISITNUM")) %>%
  mutate(proportion = count / total) %>%
  select(RSTEST, VISITNUM, RSORRES, proportion)

# Pivot the data to have one row per RSTEST and VISITNUM combination
rs_1005_proportions <- rs_1005_proportions %>%
  pivot_wider(names_from = RSORRES, values_from = proportion, values_fill = list(proportion = 0))
```

We want to plot the trajectories of each component on a barycentric plot

```{r, warning=FALSE}
plot_ternary <- function(test_component){
rs_1005_bplot <- rs_1005_proportions %>% filter(RSTEST == test_component)

plot <- ggtern(data = rs_1005_bplot, aes(x = `0`, y = `1`, z = `2`)) +
  geom_point(aes(color = factor(VISITNUM)), size = 3) + geom_jitter() +
  geom_path(arrow = arrow(type = "open", length = unit(0.15, "inches")),
            color="blue", size = 0.5) +
  theme_bw() + ggtitle(test_component) + 
  labs(x = "0", y = "1", z = "2", color = "Visit Number") +
  theme(
    plot.title = element_text(hjust = 0.5),  # Center the title
    legend.position = "right"
  )
return(plot)
}

#if (!file.exists("plots")) {
  #dir.create("plots")
#}

for (test_component in RSTEST_classes){
  plot <- plot_ternary(test_component)
  #filename <- paste("plots/", test_component, ".png", sep = "")
  #ggsave(filename, plot, device = "png")
  print(plot)
}
```

Now that we have reached a conclusion we will perform PCA using scale = TRUE!

```{r, warning=FALSE}
# apply PCA
pca_result <- prcomp(X0, center = TRUE, scale. = TRUE) 

S <- pca_result$sdev

# scree plot
plot(1:length(S), S^2, type = "b",
     xlab = "Principal Component", ylab = "Eigenvalue",
     main = "Scree Plot")

# loadings
loadings_matrix <- pca_result$rotation
# the biplot shows that hop left and right are really correlated with one another
# might be best to use one leg
biplot(pca_result, cex = 0.7, arrow.len=0.001, scale = 0.8)
print(loadings_matrix[,1:2])
```

```{r, warning=FALSE}
# nicer plots
pca_facto <- PCA(X0, graph = FALSE)

#if (!dir.exists("plots")) {
  #dir.create("plots")
#}

# Generate and save scree plot
fviz_screeplot(pca_facto, addlabels = TRUE, ylim = c(0, 100), 
               main = "Scree Plot")
#ggsave("plots/scree_plot.png")

# Generate and save biplot
fviz_pca_biplot(pca_facto, geom = "point", col.var = "contrib", 
                gradient.cols = rainbow(ncol(X0)), 
                repel = TRUE, arrow.len = 0.001, 
                label = "var", cex = 0.5)
#ggsave("plots/biplot.png")

# loadings matrix
loadings_matrix_facto <- pca_facto$var$coord

print(loadings_matrix_facto[, 1:2])
```

```{r, warning=FALSE}
# plot trajectory of the weight for each component within PC1 and PC2 against time points

num_visits = length(unique(rs_1005$VISITNUM))
num_features = length(RSTEST_list)
PC1s <- matrix(NA, nrow=num_features, ncol= num_visits)
PC2s <- matrix(NA, nrow=num_features, ncol=num_visits)
num_points <- rep.int(0, times=num_visits)

for (v in sort(unique(rs_1005$VISITNUM))){
  X <- create_matrix(rs_1005, v)
  pca <- PCA(X, graph = FALSE) 
  PC1 <- pca$var$coord[ ,1]
  PC2 <- pca$var$coord[ ,2]
  PC1s[, v+1] <- PC1
  PC2s[, v+1] <- PC2
  num_points[v+1] <- nrow(X)
}

# PC1s
plot(1:num_visits, PC1s[1, ], type = "l", col = 1, xlab = "Visit", ylab = "PC1", xlim = c(1, num_visits), ylim = range(PC1s), main = "PC1 Weights")
for (row in 2:num_features) {
  lines(1:num_visits, PC1s[row, ], col = row)
}

# Bars to illustrate the number of data points at each VISITNUM
par(new = TRUE)
barplot(height = num_points, width = 0.1, col=rgb(0.5, 0.5, 0.5, 0.5), space = 0.05, axes = FALSE)
axis(side = 4, at = axTicks(4), labels = axTicks(4))

plot(1:num_visits, PC2s[1, ], type = "l", col = 1, xlab = "Visit", ylab = "PC2", xlim = c(1, num_visits), ylim = range(PC2s), main = "PC2 Weights")

for (row in 2:num_features) {
  lines(1:num_visits, PC2s[row, ], col = row)
}

par(new = TRUE)
barplot(height = num_points, width = 0.1, col=rgb(0.5, 0.5, 0.5, 0.5), space = 0.05, axes = FALSE)
axis(side = 4, at = axTicks(4), labels = axTicks(4))
# You're looking at how each variable loads onto the first two PCs and seeing how that loading varies with time. 
# NSAA claims that we have a 1d measure of disease severity
# do not look at VISITNUM=9 because there were only 3 patients remaining
# might explain why a lot of components go to zero
# the weights are probably only reliable until the 6th visit approx
```
These plots show that when we do scale adding the components equally makes sense, but what is currently being done is not accounting for the variances of each component! maybe plot this when scale=FALSE.
```{r}
# TO DO LIST

# 1.) perform bootstrap on the gls model to see the histogram of values that the phi parameter takes.

# 2.) Perform latent class trajectory analysis on this data to see what was done in Muntoni et al - want to perform on each component trajectory as well as the trajectory of the total. 
# to evaluate maybe use cross validation so first fit the model with a certain number of clusters and find and record what cluster each trajectory is classified to
# then perform K-fold cross validation assess the accuracy - the proportion of trajectories correctly classified - average this across all K 
# we can use this average accuracy to evaluate which model is best

# cluster validation is a method which could be the same as that described above - look up the silhouette score or cosine similarity

# (Ways to deal with different sizes of trajectories (because of dropout)
# simplest - just perform analysis on complete trajectories but only 3 individuals have complete trajectories
# assign last value for the rest of time to complete incomplete trajectories
# assign average for the rest of time to complete incomplete trajectories <- this might be the best approach of a bad bunch
# I am envisioning a matrix which contains the trajectories so the values across time for each USUBJID (for a specific RSTEST) and for NA values we assign the mean of the column)

# 5.) Use the Kalman filter to perform dynamic PCA - we have weights from PCA at visit 1 then we update them for visit 2 based on the information from PCA

# 3.) Plot weights when scale=FALSE

# 4.) Plot variances of each component with time
```

We calculate the correlation parameter using gls on the total NSAA score.
```{r}
# get dataset of only NSAA
rs_1005_nsaa <- rs_1005 %>% filter(RSTEST == "NSAA1-Total")
# change the visit column so that it records integer times
# baseline and screening are given a time stamp of 0 weeks
# every other time stamp is (n-1) * 12 weeks
rs_1005_nsaa <- rs_1005_nsaa %>%
  mutate(VISIT = case_when(
    VISITNUM %in% c(0, 1) ~ 0,
    VISITNUM >= 2 & VISITNUM <= 9 ~ (VISITNUM - 1) * 12,
    TRUE ~ NA_real_
  ))

# now we can define the gls model
# we use corAR1 instead of corCAR1 because weeks would be discrete - need to ask chris about this

# remove duplicated rows
rs_1005_nsaa <- rs_1005_nsaa %>% distinct(USUBJID, VISIT, 
                                                .keep_all = TRUE)

# make RSORRES numeric
rs_1005_nsaa$RSORRES <- as.numeric(rs_1005_nsaa$RSORRES)

# output summary
nsaa_total_gls <- gls(RSORRES ~ 1 + VISIT + VISIT**2, 
                        data = rs_1005_nsaa, 
                        correlation = corAR1(form = ~ 1 | USUBJID))

# the correlation under visit is just the correlation between visit and the intercept.
summary(nsaa_total_gls)

# get the phi parameter from the data and perform bootstrap
gls_phi <- function(data, indices){
  boot_data = rs_1005_nsaa[indices, ]
  # model grouped by each patient so different patients aren't correlated
  gls_model <- gls(RSORRES ~ 1 + VISIT + VISIT**2, 
                        data = boot_data, 
                        correlation = corAR1(form = ~ 1 | USUBJID))
  # output the phi parameter
  phi <- extract_varcomp(gls_model)[1]
  return(as.numeric(phi))
}

set.seed(123) 
# 1000 bootstrap samples
bootstrap_results <- boot(data = rs_1005_nsaa, statistic = gls_phi, R = 1000)
# histogram of results
hist(bootstrap_results$t, breaks = 30, main = "Histogram of Phi Parameters",
     xlab = "Phi Parameter")
mean_phi <- mean(bootstrap_results)
print(mean_phi)
```

```{r}
# plot the residuals as a whole
plot(nsaa_total_gls, resid(., type="p")~fitted(.), abline=0)
# plot the residuals for every visit
# check if there is clustering within the residuals
# we assume no grouping when using this model - maybe if there is clustering in the residuals we might need to use a mixed model.
plot(nsaa_total_gls, resid(., type="p")~fitted(.)|VISIT, abline=0)
# qqnorm to see if standardized residuals are standard normal
qqnorm(nsaa_total_gls, abline=c(0,1))

# check autocorrelation within residuals for a random subject
residuals_data <- data.frame(residuals = residuals(nsaa_total_gls),
                             USUBJID = rs_1005_nsaa$USUBJID)
random_subject <- sample(unique(residuals_data$USUBJID), 1)
subject_residuals <- subset(residuals_data,
                            USUBJID == random_subject)$residuals

# plot PACF
# We cannot derive a correlation structure based off the PACF
# The PACF for each individual changes
# Moroever the correlation values themselves at certain lag points aren't significant enough to draw a conclusion from
pacf(subject_residuals, main=paste("PACF for Subject", random_subject))

# fit the model without ar1 correlation - then plot the residuals and check its autocorrelation.
nsaa_uncorrelated_gls <- gls(RSORRES ~ 1 + VISIT + VISIT**2,
                             data = rs_1005_nsaa)
residuals_nsaa <- residuals(nsaa_uncorrelated_gls)
# Can clearly observe a lag 1 correlation structure which is encapsulated by the AR1 in our original model.
pacf(residuals_nsaa, lag.max=10, main="PACF of Uncorrelated Residuals")
```
The summary indicates an AR(1) correlation structure in your GLS model, with a high phi estimate of 0.9639783, suggesting strong correlation between the nsaa scores of a single patient between two visits (i.e. 12 weeks apart).

The negative correlation for VISIT outputted by the summary signifies the correlation between the random effects and the error of each observation. The random effects in the model (as specified above) is the patients themselves i.e. the only grouping structure we hypothesize is groups containing one patient each. Ideally this should be zero because the error in an NSAA measurement at a particular time of a particular patient should not be correlated to the patient that the measurement comes from (random effect).

Let's interpret the standardized residuals analysis. Upon analysing both standardized residuals vs the fitted values graphs with and without the VISIT grouping structure, we can conclude that there is no obvious grouping of subjects through time i.e. not enough evidence to suggest a clustering of trajectories. The fitted values in this sense are the NSAA total scores - so one can visualise time essentially moving in the reverse direction to the fitted values such that the last vertical line of points represents the baseline measurement. We can make this deduction by examining the plots for each VISIT - by comparing the sets of plots with the "overarching" plot we can see that each VISIT make up the vertical "pillars" observed. The standardized residuals follow a standard normal distribution for each visit (as illustrated by the qqnorm plot) which backs our model specification and once again undermines the clustering claim. For an obvious grouping structure to be present we would've expected certain residual points being bunched up together at each visit point, but this is not completely obvious from the plots and would've drastically affected the qqnorm plot. 

The deliverable - if we have a treatment effect which causes an improvement in the NSAA by 'x' amount how many patients do we need in the trial?

How does clustering relate to this? If we have clusters then the model will change and the treatment effect may be different between clusters - so we performed clustering as a sense check to see if there is a meaningful way to separate the patients - idea for further work.

```{r}
# LCTM work
rs_1005_nsaa$USUBJID_numeric <- as.numeric(factor(rs_1005_nsaa$USUBJID))
lctm_4 <- hlme(fixed = RSORRES ~ 1 + VISIT + VISIT**2,
               mixture = ~ 1 + VISIT + VISIT**2, 
               subject = 'USUBJID_numeric',
               ng = 4,
               data = rs_1005_nsaa)

averages <- rs_1005_nsaa %>%
  filter(VISIT == 0) %>%
  group_by(USUBJID) %>%
  summarise(RSORRES = mean(RSORRES, na.rm = TRUE))

rs_1005_nsaa <- rs_1005_nsaa %>%
  left_join(averages, by = "USUBJID", suffix = c("", "_avg")) %>%
  mutate(RSORRES = ifelse(VISIT == 0, RSORRES_avg, RSORRES)) %>%
  select(-RSORRES_avg)

lctm_data <- rs_1005_nsaa %>%
  arrange(USUBJID, VISIT) %>%
  pivot_wider(id_cols = USUBJID, 
              names_from = VISIT, 
              values_from = RSORRES, 
              names_prefix = "VISIT")
```