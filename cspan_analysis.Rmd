---
title: "dmd-cspan-analysis"
author: "Vignesh Balaji"
date: "2024-03-25"
output:
  pdf_document: default
  html_document: default
---

```{r setup, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, include=FALSE}
library(pheatmap)
library(FactoMineR)
library(readr)
library(tidyverse)
library(dplyr)
library(ggtern)
library(factoextra)
library(nlme)
library(forecast)
library(lcmm)
library(lmeInfo)
library(boot)
library(xtable)
library(gridExtra)
library(ggrepel)
rs <- read_csv("rs.csv")
```

We perform some data analysis. Firstly, we only obtain data which belong to the study DMD-1005. We then filter so that we only get values of each NSAA component at baseline for each patient. The VISITNUM are in a nonsensical order because some patients are assigned a visit number of 99. This has to be investigated - hypothesis is that the patients which were assigned a VISITNUM of 99 did not make it past screening.

```{r, cache=TRUE}
# get only the relevant study
rs_1005 <- rs %>% filter(STUDYID == "DMD-1005")

# find all patients that participated in the study before screening
rs_1005_base <- rs_1005 %>% filter(VISITNUM == 0)

base_patients <- unique(rs_1005_base$USUBJID)

# find all patients that participated in the study before screening
rs_1005_base_check <- rs_1005 %>% filter(RSTEST == "NSAA1-Total", VISITNUM==0) %>% distinct(USUBJID, RSTEST, .keep_all = TRUE) 

# this should be the same length as base_patients
base_patients_check <- rs_1005_base_check$USUBJID

# find all patients that were screened
rs_1005_screened <- rs_1005 %>% filter(VISITNUM == 1)

screened_patients <- unique(rs_1005_screened$USUBJID)

# check if all base patients began screening
print(length(intersect(base_patients, screened_patients)))
# yes they did

# find all patients that weren't removed from screening
rs_1005_begin <- rs_1005 %>% filter(VISITNUM == 4)

begin_patients <- unique(rs_1005_begin$USUBJID)

# check if all base patients began the study
print(length(intersect(base_patients, begin_patients)))
# three patients failed screening

failed_screen <- setdiff(base_patients, begin_patients)

# find all patients that dropped out
rs_1005_end <- rs_1005 %>% filter(VISITNUM == 99)

dropout_patients = unique(rs_1005_end$USUBJID)

# investigate how many of the patients that failed screening were given VN = 99
print(length(intersect(failed_screen, dropout_patients)))
# none of them
# this is good
# All patients that dropped out therefore will have VISITNUM >= 5
```

Idea: Iterate through the dropped out patients. Find their second largest visit number (x), as their largest will be 99, then change VISITNUM = 99 to VISITNUM = x+1

```{r, cache=TRUE}
for(patient_id in dropout_patients) {
  # Find the second largest VISITNUM for the current patient excluding 99
  second_largest_visitnum <- rs_1005 %>%
    filter(USUBJID == patient_id, VISITNUM != 99) %>%
    arrange(desc(VISITNUM)) %>%
    slice(1) %>%
    pull(VISITNUM)
  
  # Check if there is a valid second largest VISITNUM
  if(length(second_largest_visitnum) > 0) {
    # Update the VISITNUM from 99 to second largest + 1
    rs_1005 <- rs_1005 %>%
      mutate(VISITNUM = if_else(USUBJID == patient_id & VISITNUM == 99, second_largest_visitnum + 1, VISITNUM))
  }
}

# This is just so that the visit numbers are from 0-9
mapping <- setNames(c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), c(0, 1, 4, 5, 6, 7, 8, 9, 10, 11))

rs_1005$VISITNUM <- mapping[as.character(rs_1005$VISITNUM)]
```

Now for some EDA
```{r}
rs_1005_distinct <- rs_1005 %>%
  filter(RSTEST == "NSAA1-Total") %>%
  distinct(USUBJID, VISITNUM, RSORRES, .keep_all = TRUE)

rs_plot <- rs_1005_distinct %>%
  filter(RSTEST == "NSAA1-Total") %>%
  group_by(USUBJID) %>%
  mutate(last_visit = VISITNUM == max(VISITNUM)) %>%
  ungroup()

dropoutplot <- ggplot(data = rs_plot, aes(x = VISITNUM, y = as.numeric(RSORRES), group = USUBJID)) +
  geom_line(color = "gray", size = 1) +
  geom_point(aes(color = ifelse(last_visit, "dropout", "active")), size = 2, shape = 21, fill = ifelse(rs_plot$last_visit, "red", "blue")) +
  scale_color_manual(values = c("active" = "blue", "dropout" = "red"),
                     labels = c("active" = "Active", "dropout" = "Dropout")) +
  scale_x_continuous(breaks = c(0, 3, 6, 9)) +
  labs(title = "Patient Trajectories with Dropout",
       x = "Visit Number",
       y = "NSAA Total Score",
       color = "Patient Status") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(dropoutplot)
ggsave("plots/dropoutplot.png", dropoutplot, dpi=300)
```


Create table of means and s.d. for each visit
```{r}
rs_1005_distinct$RSORRES <- as.numeric(rs_1005_distinct$RSORRES)

# Summarize data: calculate mean, sd, and count for each VISITNUM
summary_stats <- rs_1005_distinct %>%
  filter(RSTEST == "NSAA1-Total") %>%
  group_by(VISITNUM) %>%
  summarise(
    mean = mean(RSORRES, na.rm = TRUE),
    sd = sd(RSORRES, na.rm = TRUE),
    n = n(),  # Counting the number of observations at each visit
    .groups = 'drop'  # Drop grouping after summarization
  ) %>%
  mutate(label = sprintf("%.2f (%.2f), n = %d", mean, sd, n))  # Format mean, sd, and n into a single string

print(summary_stats)
```

Visualise the different stages of disease.
```{r, warning=FALSE}
# choose three patients who are at different stages
# DMD-1005/508-80853 - decreasing
# DMD-1005/018-87850 - increasing
# DMD-1005/018-87881 - constant

specific_patients <- rs_plot %>% filter(USUBJID %in% c("DMD-1005/018-87850", "DMD-1005/018-87881"))

diff_stagesplot <- ggplot(data = specific_patients, aes(x = VISITNUM, y = as.numeric(RSORRES), group = USUBJID)) +
  geom_line(color = "gray", size = 1) +
  geom_point(aes(color = ifelse(last_visit, "dropout", "active")), size = 2, shape = 21, fill = ifelse(specific_patients$last_visit, "red", "blue")) +
  scale_color_manual(values = c("active" = "blue", "dropout" = "red"),
                     labels = c("active" = "Active", "dropout" = "Dropout")) +
  scale_x_continuous(breaks = c(0, 3, 6, 9)) +
  labs(title = "Specific Patient Trajectories",
       x = "Visit Number",
       y = "NSAA Total Score",
       color = "Patient Status") +
  theme_minimal() +
  theme(legend.position = "bottom")

ggsave("plots/diffstagesplot.png", diff_stagesplot, height=5)
```

Now we get the matrix that we input into PCA from this
```{r,cache=TRUE}
# first we have to investigate all the factors that make up RSTEST column

RSTEST_classes <- unique(rs_1005$RSTEST)

# remove NSAA1_Total and NSAA1_Total Linearized
totals_to_remove <- which(RSTEST_classes %in% c("NSAA1-Total", "NSAA1-Total Linearized"))

RSTEST_classes <- RSTEST_classes[-totals_to_remove]

# also remove any grades, timing and velocity results
keywords_to_remove <- c("Velocity", "Time", "Grade")
misc_to_remove <- grep(paste0(paste(keywords_to_remove, collapse = "|"), collapse = "|"), RSTEST_classes)

RSTEST_classes <- RSTEST_classes[-misc_to_remove]

RSTEST_list <- as.list(RSTEST_classes)

rs_1005_restricted <- rs_1005_base %>% filter(RSTEST %in% RSTEST_list)

# remove duplicated rows
rs_1005_data <- rs_1005_restricted %>% distinct(USUBJID, RSTEST, 
                                                .keep_all = TRUE)

# use pivot wider to create the desired df
rs_1005_pivot <- rs_1005_data %>% select(USUBJID, RSTEST, RSORRES) %>%
  pivot_wider(id_cols = USUBJID,
              names_from = RSTEST,
              values_from = RSORRES)
```

Let's now extract the matrix and create a function which generalises this process for any VISITNUM.

```{r,cache=TRUE}
# convert to matrix removing the first column as that is subject ID
X0 <- as.matrix(rs_1005_pivot[2:18])
# fill na with mean value of the column
X0 <- apply(X0, 2, as.numeric)
column_means <- colMeans(X0, na.rm = TRUE) 
X0[is.na(X0)] <- column_means[rep(1:ncol(X0), each = nrow(X0))]

# define a function which does this for any VISITNUM
create_matrix <- function(df, VN){
  df_new <- df %>% filter(VISITNUM == VN)
  RSTEST_classes <- unique(df$RSTEST)
  totals_to_remove <- which(RSTEST_classes %in% c("NSAA1-Total", "NSAA1-Total Linearized"))
  RSTEST_classes <- RSTEST_classes[-totals_to_remove]
  keywords_to_remove <- c("Velocity", "Time", "Grade")
  misc_to_remove <- grep(paste0(paste(keywords_to_remove, collapse = "|"), collapse = "|"), RSTEST_classes)

  RSTEST_classes <- RSTEST_classes[-misc_to_remove]

  RSTEST_list <- as.list(RSTEST_classes)

  df_restricted <- df_new %>% filter(RSTEST %in% RSTEST_list)
  df_data <- df_restricted %>% distinct(USUBJID, RSTEST, 
                                                .keep_all = TRUE)
  df_pivot <- df_data %>% select(USUBJID, RSTEST, RSORRES) %>%
  pivot_wider(id_cols = USUBJID,
              names_from = RSTEST,
              values_from = RSORRES)
  
  X <- as.matrix(df_pivot[2:18])
  X <- apply(X, 2, as.numeric)
  column_means <- colMeans(X, na.rm = TRUE) 
  X[is.na(X)] <- column_means[rep(1:ncol(X), each = nrow(X))]
  return(X)
}
```

Now we perform PCA, after constructing our matrix.

Should we use scale = TRUE or scale = FALSE? scale = TRUE will standardize each feature or variable before we perform PCA. This can be particularly useful in certain cases, for instance if one variable takes larger values it will exhibit larger variance by nature, this then translates to it potentially falsely dominating PC1. To combat this, we standardize the data so that we have a “global” scale for all variables, i.e. the analysis will be invariant to the measurement scale.

In this setting, we have that each component of the NSAA takes a value of 0, 1 or 2 and so are on the same scale. However, one task may be inherently easier to do than the other, and so has an abnormally higher frequency of 2s. So, to answer the question scale = TRUE or FALSE, we have to investigate the proportion of 0s, 1s and 2s for each component (outputted as barycentric coordinates), as well as the variance-covariance matrix to formulate a robust conclusion.

```{r, cache=TRUE}
# create correlation heatmap at baseline
cov_matrix <- cor(X0)
corrheatmap <- pheatmap(cov_matrix, scale='none')

ggsave("plots/corrheatmap.png", corrheatmap, height=10, width=8, dpi=300)
```

```{r}
colnames(X0) <- gsub("NSAA1-", "", colnames(X0))

# Create data frame with updated column names
feature_stats <- data.frame(
  feature = colnames(X0),
  prop_mean = apply(X0, 2, mean) / 2,
  prop_mean2 = (apply(X0, 2, mean) / 2)**2,
  variance = apply(X0, 2, var)
)

# Define the range of means to plot the Bernoulli relationship over
mean_range <- seq(min(feature_stats$prop_mean), max(feature_stats$prop_mean), length.out = length(feature_stats$prop_mean))

# find the best fitting curve by plotting variance against p(1-p) because they are proportional

mvar_lm <- lm(variance ~ prop_mean + prop_mean2, data = feature_stats)
summary(mvar_lm)
# optimal dispersion parameter is roughly 2.
```

```{r}
bernoulli_variance <- 2 * mean_range * (1 - mean_range)  # Bernoulli variance across the range

# Create scatter plot with actual data and Bernoulli relationship
mvarplot <- ggplot(feature_stats, aes(x = prop_mean, y = variance, label = feature)) +
  geom_point() +  # Actual data points
  geom_label_repel(nudge_y = 0.05, box.padding = 0.35, point.padding = 0.5, segment.color = 'grey50') +
  geom_line(aes(x = mean_range, y = bernoulli_variance, color = "Bin(2, p) Mean-Variance Relationship"), linetype = "dashed") + # Expected Bernoulli relationship
  labs(x = "Mean Proportion", y = "Variance", title = "Mean-Variance Relationship of each NSAA Item") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(title = NULL))

# Save the plot
ggsave("plots/mvarplot.png", mvarplot, width = 8, height = 6)

print(mvarplot)
```

```{r, cache=TRUE}
# Calculate the counts for each RSORRES value within each RSTEST and VISITNUM group
rs_1005_counts <- rs_1005 %>%
  filter(RSTEST %in% RSTEST_list) %>%
  group_by(RSTEST, VISITNUM, RSORRES) %>%
  summarise(count = n(), .groups = 'drop')

# Calculate the total counts for each RSTEST and VISITNUM group
rs_1005_totals <- rs_1005_counts %>%
  group_by(RSTEST, VISITNUM) %>%
  summarise(total = sum(count), .groups = 'drop')

# Join the counts with the totals to calculate proportions
rs_1005_proportions <- rs_1005_counts %>%
  left_join(rs_1005_totals, by = c("RSTEST", "VISITNUM")) %>%
  mutate(proportion = count / total) %>%
  select(RSTEST, VISITNUM, RSORRES, proportion)

# Pivot the data to have one row per RSTEST and VISITNUM combination
rs_1005_proportions <- rs_1005_proportions %>%
  pivot_wider(names_from = RSORRES, values_from = proportion, values_fill = list(proportion = 0))
```

We want to plot the trajectories of each component on a barycentric plot

```{r, warning=FALSE, cache=TRUE}
plot_ternary <- function(test_component){
rs_1005_bplot <- rs_1005_proportions %>% filter(RSTEST == test_component)

plot <- ggtern(data = rs_1005_bplot, aes(x = `0`, y = `1`, z = `2`)) +
  geom_point(aes(color = factor(VISITNUM)), size = 3) + geom_jitter() +
  geom_path(arrow = arrow(type = "open", length = unit(0.15, "inches")),
            color="blue", size = 0.5) +
  theme_bw() + ggtitle(test_component) + 
  labs(x = "0", y = "1", z = "2", color = "Visit Number") +
  theme(
    plot.title = element_text(hjust = 0.5),  # Center the title
    legend.position = "right"
  )
return(plot)
}

#if (!file.exists("plots")) {
  #dir.create("plots")
#}

#for (test_component in RSTEST_classes){
  #plot <- plot_ternary(test_component)
  #filename <- paste("plots/", test_component, ".png", sep = "")
  #ggsave(filename, plot, device = "png")
  #print(plot)
#}
```

Now that we have reached a conclusion we will perform PCA using scale = TRUE!

```{r, warning=FALSE}
# apply PCA
pca_result <- prcomp(X0, scale. = TRUE) 

S <- pca_result$sdev

# scree plot
plot(1:length(S), S^2, type = "b",
     xlab = "Principal Component", ylab = "Eigenvalue",
     main = "Scree Plot")

# loadings
loadings_matrix <- pca_result$rotation
# the biplot shows that hop left and right are really correlated with one another
# might be best to use one leg
biplot(pca_result, cex = 0.7, arrow.len=0.001, scale = 0.8)
print(loadings_matrix[,1:2])
```

```{r, warning=FALSE}
# nicer plots
pca_facto <- PCA(X0, graph = FALSE, scale.unit=FALSE)

#if (!dir.exists("plots")) {
  #dir.create("plots")
#}

# Generate and save scree plot
my_screeplot <- fviz_screeplot(pca_facto, addlabels = TRUE, ylim = c(0, 100), 
               main = "Scree Plot")
ggsave("plots/scree_plot_raw.png", my_screeplot, width=10, height=8, dpi=300)

# Generate and save biplot
my_biplot <- fviz_pca_biplot(pca_facto, geom = "point", col.var = "contrib", 
                gradient.cols = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf"), 
                repel = TRUE, arrow.len = 0.001, 
                label = "var", cex = 0.5, pointsize = 2, labelsize = 5, 
                repel.box.padding = 0.2, repel.segment.size = 0.3)

ggsave("plots/biplot_raw.png", my_biplot, width = 10, height = 8, dpi = 300)

# loadings matrix
loadings_matrix_facto <- pca_facto$var$coord

print(loadings_matrix_facto[, 1:2]) # loadings are a factor of e off prcomp - ask chris about this
```

```{r, warning=FALSE}
# plot trajectory of the weight for each component within PC1 and PC2 against time points

num_visits =  6 #length(unique(rs_1005$VISITNUM))
num_features = length(RSTEST_list)
PC1s <- matrix(NA, nrow=num_features, ncol=num_visits)
PC2s <- matrix(NA, nrow=num_features, ncol=num_visits)
num_points <- rep.int(0, times=num_visits)
#sort(unique(rs_1005$VISITNUM)
for (v in 0:5){
  X <- create_matrix(rs_1005, v)
  pca <- PCA(X, graph = FALSE) 
  PC1 <- pca$var$coord[ ,1]
  PC2 <- pca$var$coord[ ,2]
  PC1s[, v+1] <- PC1
  PC2s[, v+1] <- PC2
  num_points[v+1] <- nrow(X)
}

png("plots/PC1s_scaled.png", width = 5000, height = 2000, res = 300)
par(mar = c(5, 4, 4, 2) + 0.1, oma = c(0, 0, 0, 18), cex.lab=1.6, cex.main=2.4)
# PC1s
plot(1:num_visits, PC1s[1, ], type = "l", col = 1, xlab = "Visit", ylab = "PC1", xlim = c(1, num_visits), ylim = range(PC1s), main = "PC1 Weights (Scaled)")
for (row in 2:num_features) {
  lines(1:num_visits, PC1s[row, ], col = row)
}
legend("right", inset = -0.3, legend = RSTEST_list, col = 1:num_features, lty = 1, xpd = NA, cex = 1)
dev.off()

# Bars to illustrate the number of data points at each VISITNUM
#par(new = TRUE)
#barplot(height = num_points, width = 0.01, col=rgb(0.5, 0.5, 0.5, 0.5), space = 5, axes = FALSE)
#axis(side = 4, at = axTicks(4), labels = axTicks(4))
#mtext("Sample Size", side=4, line=3)

png("plots/PC2s_scaled.png", width = 5000, height = 2000, res = 300)
par(mar = c(5, 4, 4, 2) + 0.1, oma = c(0, 0, 0, 18), cex.lab=1.6, cex.main=2.4)
plot(1:num_visits, PC2s[1, ], type = "l", col = 1, xlab = "Visit", ylab = "PC2", xlim = c(1, num_visits), ylim = range(PC2s), main = "PC2 Weights (Scaled)")

for (row in 2:num_features) {
  lines(1:num_visits, PC2s[row, ], col = row)
}
legend("right", inset = -0.3, legend = RSTEST_list, col = 1:num_features, lty = 1, xpd = NA, cex = 1)
dev.off()

# You're looking at how each variable loads onto the first two PCs and seeing how that loading varies with time. 
# NSAA claims that we have a 1d measure of disease severity
```
The distribution of principal component weights of each component at a specific visit number has a mean of 0.7, but it is clear to see some outliers and the variance in the distribution increase with visit. Therefore, it is hard to justify adding each component equally to give a total NSAA score, even after taking into account that some components display different variability. A potential approach to combat this is to set the total score to be a linear combination of the components, where each scale factor is the PC1 weight of each item.

```{r}
# plot trajectory of the weight for each component within PC1 and PC2 against time points

num_visits =  6 #length(unique(rs_1005$VISITNUM))
num_features = length(RSTEST_list)
PC1s_f <- matrix(NA, nrow=num_features, ncol=num_visits)
PC2s_f <- matrix(NA, nrow=num_features, ncol=num_visits)
num_points <- rep.int(0, times=num_visits)
#sort(unique(rs_1005$VISITNUM)
for (v in 0:5){
  X <- create_matrix(rs_1005, v)
  pca <- PCA(X, graph = FALSE, scale.unit = FALSE) 
  PC1 <- pca$var$coord[ ,1]
  PC2 <- pca$var$coord[ ,2]
  PC1s_f[, v+1] <- PC1
  PC2s_f[, v+1] <- PC2
  num_points[v+1] <- nrow(X)
}

png("plots/PC1s_unscaled.png", width = 5000, height = 2000, res = 300)
par(mar = c(5, 4, 4, 2) + 0.1, oma = c(0, 0, 0, 18), cex.lab=1.6, cex.main=2.4)
# PC1s
plot(1:num_visits, PC1s_f[1, ], type = "l", col = 1, xlab = "Visit", ylab = "PC1", xlim = c(1, num_visits), ylim = range(PC1s_f), main = "PC1 Weights (Raw)")
for (row in 2:num_features) {
  lines(1:num_visits, PC1s_f[row, ], col = row)
}
legend("right", inset = -0.3, legend = RSTEST_list, col = 1:num_features, lty = 1, xpd = NA, cex = 1)
dev.off()

png("plots/PC2s_unscaled.png", width = 5000, height = 2000, res = 300)
par(mar = c(5, 4, 4, 2) + 0.1, oma = c(0, 0, 0, 18), cex.lab=1.6, cex.main=2.4)
# PC1s
plot(1:num_visits, PC2s_f[1, ], type = "l", col = 1, xlab = "Visit", ylab = "PC1", xlim = c(1, num_visits), ylim = range(PC2s_f), main = "PC1 Weights (Raw)")
for (row in 2:num_features) {
  lines(1:num_visits, PC2s_f[row, ], col = row)
}
legend("right", inset = -0.3, legend = RSTEST_list, col = 1:num_features, lty = 1, xpd = NA, cex = 1)
dev.off()
```
When we don't apply scaling to our principal component analysis, we see a very sparse distribution in the weights of both principal components through time. This further damages the rationale behind adding up the components equally to give a total NSAA score out of 34.

We calculate the correlation parameter using gls on the total NSAA score.
```{r}
# get dataset of only NSAA
rs_1005_nsaa <- rs_1005 %>% filter(RSTEST == "NSAA1-Total")
# change the visit column so that it records integer times
# baseline and screening are given a time stamp of 0 weeks
# every other time stamp is (n-1) * 12 weeks
rs_1005_nsaa <- rs_1005_nsaa %>%
  mutate(VISIT = case_when(
    VISITNUM %in% c(0, 1) ~ 0,
    VISITNUM >= 2 & VISITNUM <= 9 ~ (VISITNUM - 1) * 12,
    TRUE ~ NA_real_
  ))

# now we can define the gls model
# remove duplicated rows
rs_1005_nsaa <- rs_1005_nsaa %>% distinct(USUBJID, VISIT, 
                                                .keep_all = TRUE)

# make RSORRES numeric
rs_1005_nsaa$RSORRES <- as.numeric(rs_1005_nsaa$RSORRES)

# output summary
nsaa_total_gls <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2), 
                        data = rs_1005_nsaa, 
                        correlation = corAR1(form = ~ 1 | USUBJID))

# the correlation under visit is just the correlation between visit coefficient and the intercept.
summary(nsaa_total_gls)
```

Perform bootstrapping
```{r}
# manually perform bootstrap to get the autocorrelation parameter
patient_ids <- unique(rs_1005_nsaa$USUBJID)
boot_phis <- rep(0, 1000)

# define function that gets the phi parameter in a boot sample
gls_phi <- function(patient_ids){
  # choose only the patients that appear in the bootstrap sample
  boot_data = rs_1005_nsaa %>% filter(USUBJID %in% unique(patient_ids))
  # model grouped by each patient so different patients aren't correlated
  gls_model <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2), 
                        data = boot_data, 
                        correlation = corAR1(form = ~ 1 | USUBJID))
  # output the phi parameter
  phi <- extract_varcomp(gls_model)[1]
  return(as.numeric(phi))
}

for (reps in 1:1000){
  # sample with replacement from patient_ids
  boot_indices <- sample(1:131, size = 131, replace = TRUE)
  boot_patients <- patient_ids[boot_indices]
  boot_phi <- gls_phi(boot_patients)
  boot_phis[reps] <- boot_phi
}

# 1000 bootstrap samples
# histogram of results
phi_data <- data.frame(Phi = boot_phis)
phi_histogram <- ggplot(phi_data, aes(x = Phi)) + 
  geom_histogram(bins = 30, fill = "lightblue", color = "black") +
  ggtitle("Histogram of Phi Parameters") +
  xlab("Phi Parameter") +
  ylab("Frequency") +
  theme_minimal()

ggsave("plots/phihistogram.png", phi_histogram)
mean_phi <- mean(boot_phis)
print(mean_phi)
```

```{r}
gls_coeffs <- function(patient_ids) {
  # Filter data for the current bootstrap sample
  boot_data <- rs_1005_nsaa %>% filter(USUBJID %in% unique(patient_ids))
  
  # Fit the GLS model
  gls_model <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2), 
                   data = boot_data, 
                   correlation = corAR1(form = ~ 1 | USUBJID))
  
  # Extract and return coefficients, including the intercept
  coeffs <- coef(gls_model)
  return(coeffs)
}

boot_coeffs <- data.frame(Intercept = double(1000), VISIT = double(1000), VISIT_squared = double(1000))

for (reps in 1:1000) {
  # Sample with replacement from patient_ids
  boot_indices <- sample(patient_ids, size = length(patient_ids), replace = TRUE)
  
  # Get coefficients for the bootstrap sample
  coeffs <- gls_coeffs(boot_indices)
  
  # Store coefficients
  boot_coeffs$Intercept[reps] <- coeffs['(Intercept)']
  boot_coeffs$VISIT[reps] <- coeffs['VISIT']
  boot_coeffs$VISIT_squared[reps] <- coeffs['I(VISIT^2)']
}

p0 <- ggplot(boot_coeffs, aes(x = Intercept)) + 
  geom_histogram(bins = 30, fill = "red", color = "black") +
  ggtitle("Histogram of Intercept Estimates") +
  xlab("Intercept") +
  ylab("Frequency") +
  theme_minimal()
p1 <- ggplot(boot_coeffs, aes(x = VISIT)) + 
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  ggtitle("Histogram of Linear Coefficient Estimates") +
  xlab("Coefficient of VISIT") +
  ylab("Frequency") +
  theme_minimal()
p2 <- ggplot(boot_coeffs, aes(x = VISIT_squared)) + 
  geom_histogram(bins = 30, fill = "green", color = "black") +
  ggtitle("Histogram of Quadratic Coefficient Estimates") +
  xlab("Coefficient of VISIT Squared") +
  ylab("Frequency") +
  theme_minimal()

# Save the plot if needed
ggsave("plots/intercept_histogram.png", p0)
ggsave("plots/visit_coeff_histogram.png", p1)
ggsave("plots/visit_squared_coeff_histogram.png", p2)


print(p0)
print(p1)
print(p2)

# Statistical summary for the intercept
mean_intercept <- mean(boot_coeffs$Intercept)
mean_visit <- mean(boot_coeffs$VISIT)
mean_visit_squared <- mean(boot_coeffs$VISIT_squared)
print(paste("Mean Intercept:", mean_intercept))
print(paste("Mean Coefficient for VISIT:", mean_visit))
print(paste("Mean Coefficient for VISIT Squared:", mean_visit_squared))
```



```{r}
# plot the residuals as a whole
plot(nsaa_total_gls, resid(., type="p")~fitted(.), abline=0)
# plot the residuals for every visit
# check if there is clustering within the residuals
# we assume no grouping when using this model - maybe if there is clustering in the residuals we might need to use a mixed model.
plot(nsaa_total_gls, resid(., type="p")~fitted(.)|VISIT, abline=0)
# qqnorm to see if standardized residuals are standard normal
qqnorm(nsaa_total_gls, abline=c(0,1))

# check autocorrelation within residuals for a random subject
residuals_data <- data.frame(residuals = residuals(nsaa_total_gls),
                             USUBJID = rs_1005_nsaa$USUBJID)
random_subject <- sample(unique(residuals_data$USUBJID), 1)
subject_residuals <- subset(residuals_data,
                            USUBJID == random_subject)$residuals

# plot PACF
# We cannot derive a correlation structure based off the PACF
# The PACF for each individual changes
# Moroever the correlation values themselves at certain lag points aren't significant enough to draw a conclusion from
pacf(subject_residuals, main=paste("PACF for Subject", random_subject))

# fit the model without ar1 correlation - then plot the residuals and check its autocorrelation.
nsaa_uncorrelated_gls <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2),
                             data = rs_1005_nsaa)
residuals_nsaa <- residuals(nsaa_uncorrelated_gls)
# Can clearly observe a lag 1 correlation structure which is encapsulated by the AR1 in our original model.
plot(residuals_nsaa)
pacf(residuals_nsaa, lag.max=10, main="PACF of Uncorrelated Residuals")
```

The summary indicates an AR(1) correlation structure in our GLS model, with a high phi estimate of 0.9639783, suggesting strong correlation between the nsaa scores of a single patient between two visits (i.e. 12 weeks apart). This is as expected because someone who scores highest at visit 1 (arbitrary) is likely to score highest at visit 2, regardless of the stage of disease. Correlation is independent of the trend.

Let's interpret the standardized residuals analysis. Upon analysing both standardized residuals vs the fitted values graphs with and without the VISIT grouping structure, we can conclude that there is no obvious grouping of subjects through time i.e. not enough evidence to suggest a clustering of trajectories. The fitted values in this sense are the NSAA total scores - so one can visualise time essentially moving in the reverse direction to the fitted values such that the last vertical line of points represents the baseline measurement. We can make this deduction by examining the plots for each VISIT - by comparing the sets of plots with the "overarching" plot we can see that each VISIT make up the vertical "pillars" observed. The standardized residuals follow a standard normal distribution for each visit (as illustrated by the qqnorm plot) which backs our model specification and once again undermines the clustering claim. For an obvious grouping structure to be present we would've expected certain residual points being bunched up together at each visit point, but this is not completely obvious from the plots and would've drastically affected the qqnorm plot. 

The deliverable - if we have a treatment effect which causes an improvement in the NSAA by 'x' amount how many patients do we need in the trial?

How does clustering relate to this? If we have clusters then the model will change and the treatment effect may be different between clusters - so we performed clustering as a sense check to see if there is a meaningful way to separate the patients - idea for further work.

```{r, cache=TRUE}
# LCTM work
rs_1005_nsaa$USUBJID_numeric <- as.numeric(factor(rs_1005_nsaa$USUBJID))

lctm_1 <- hlme(fixed = RSORRES ~ VISIT + I(VISIT^2), 
               subject = 'USUBJID_numeric',
               ng = 1,
               data = rs_1005_nsaa)
lctm_2 <- gridsearch(hlme(fixed = RSORRES ~ VISIT + I(VISIT^2),
                          mixture =~VISIT + I(VISIT^2),
                          ng = 2,
                          subject = 'USUBJID_numeric',
                          data = rs_1005_nsaa),
                     rep=100, maxiter=30, minit=lctm_1)
lctm_3 <- gridsearch(hlme(fixed = RSORRES ~ VISIT + I(VISIT^2),
                          mixture =~VISIT + I(VISIT^2),
                          ng = 3,
                          subject = 'USUBJID_numeric',
                          data = rs_1005_nsaa),
                     rep=100, maxiter=30, minit=lctm_1)
lctm_4 <- gridsearch(hlme(fixed = RSORRES ~ VISIT + I(VISIT^2),
                          mixture =~VISIT + I(VISIT^2),
                          ng = 4,
                          subject = 'USUBJID_numeric',
                          data = rs_1005_nsaa),
                     rep=100, maxiter=30, minit=lctm_1)
```

```{r}
summarytable(lctm_1, lctm_2, lctm_3, lctm_4)
summaryplot(lctm_1, lctm_2, lctm_3, lctm_4)
```
We will investigate how the size of the treatment effect affects its significance in the model
```{r}
treatment_effect_fn <- function(data, num_bootstraps) {
  p_values <- numeric(num_bootstraps)
  patient_ids <- unique(data$USUBJID)  # Get unique patient IDs once

  for (i in 1:num_bootstraps) {
    # Bootstrap: Sample patient IDs with replacement
    boot_patients <- sample(patient_ids, size = length(patient_ids), replace = TRUE)
    
    # Filter data to only include sampled patient IDs
    sampled_data <- data %>% filter(USUBJID %in% boot_patients)
    
    # Randomly select half of the bootstrap sample for treatment
    selected_patients <- sample(boot_patients, size = length(boot_patients) / 2)
    
    # Assign treatment of 5 and adjust scores
    sampled_data <- sampled_data %>%
      mutate(TREAT = ifelse(USUBJID %in% selected_patients, 1, 0),
             RSORRES = ifelse(TREAT == 1, RSORRES + 5, RSORRES))
    
    # Fit the GLS model
    gls_model <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2) + TREAT, data = sampled_data, 
                     correlation = corAR1(form = ~ 1 | USUBJID))
    summary_gls <- summary(gls_model)
    
    # Extract p-value
    coefficients <- summary_gls$tTable[, "Value"]
    standard_errors <- summary_gls$tTable[, "Std.Error"]
    z_scores <- coefficients / standard_errors
    p_values[i] <- 2 * pnorm(abs(z_scores["TREAT"]), lower.tail = FALSE)
  }
  
  return(p_values)
}

set.seed(123)
bootstrap_p_values <- treatment_effect_fn(rs_1005_nsaa, num_bootstraps = 1000)
sig_values <- sum(bootstrap_p_values < 0.05) / length(bootstrap_p_values)

# Plot the p-values
ggplot(data.frame(PValue = bootstrap_p_values), aes(x = PValue)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of P-Values from Bootstrap", x = "P-Value", y = "Frequency")

print(sig_values)

```
Here we print out the proportion of significant p-values (i.e. p-values under 0.05). This is an unbiased estimator for the statistical power of the hypothesis test we are conducting (the hypothesis test is essentially is there enough evidence to say that the mean trajectory of an untreated and treated patient is the same), which is a test to investigate whether the treatment effect has an observable impact on the mean NSAA trajectories. When the proportion of significant p-values is close to 1 this illustrates we have a strong statistical test, which translates to the probability that the test correctly rejects the null hypothesis (in this scenario this is the hypothesis that the treatment effect has no observable impact). Therefore, if we have a proportion for \verb|sig_values| close to 1 then if there is indeed a treatment effect the test is high likely to detect it. Conversely, if it is closer to 0 then the test is unlikely to detect the treatment effect. We can see that a treatment effect of 5 gives us our desired result (a proportion > 0.8) for a sample size of 131 patients. Now we must simulate how the sample size affects the power.

```{r}
rho <- mean_phi # 0.96
sigma <- 5 # modify std

# Parameters
N <- 100 # Number of patients
visits <- 1:10 # Visits from 1 to 10
treatment_effect <- 5

# Using rough numbers
intercept <- 10
visit_coef <- 2.5
visit_squared_coef <- -0.5

# Ensure half of the patients are treated (random)
#treatment_assignments <- sample(c(rep(1, N/2), rep(0, N/2)))


# Ensure first half of patients are treated - for t test purposes
treatment_assignments <- ifelse(1:N <= N/2, 1, 0)

# Simulate AR(1) residuals for one patient (10 visits)
simulate_ar1_residuals <- function(n, rho, sigma) {
  e <- rnorm(n, mean = 0, sd = sigma)
  residuals <- vector(length = n)
  residuals[1] <- e[1]
  
  for (i in 2:n) {
    residuals[i] <- rho * residuals[i - 1] + e[i]
  }
  
  return(residuals)
}

# Function to generate data for one patient
simulate_patient_data <- function(patient_id, treat, visits, rho, sigma) {
  n <- length(visits)
  ar1_residuals <- simulate_ar1_residuals(n, rho, sigma)
  
  rsorres <- intercept + 
             visit_coef * visits + 
             visit_squared_coef * visits^2 + 
             treat * treatment_effect +
             ar1_residuals
  
  # Ensure correct range and integer type
  rsorres <- round(pmin(pmax(rsorres, 0), 34))
  
  data.frame(
    USUBJID = rep(patient_id, n),
    VISIT = visits,
    TREAT = rep(treat, n),
    RSORRES = rsorres
  )
}

# Simulate data for N patients and 10 visits
patient_data <- lapply(1:N, function(i) {
  treat <- sample(c(0,1), 1)
  simulate_patient_data(i, treat, visits, rho, sigma)
})

# Combine all patient data into one data frame
simulated_data <- bind_rows(patient_data)

gls_model <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2) + TREAT, 
                      data = simulated_data, 
                      correlation = corAR1(form = ~ 1 | USUBJID))
summary_gls <- summary(gls_model)
print(summary_gls)
```

```{r}
# Function to perform bootstrap analysis
bootstrap_analysis <- function(data, num_bootstraps) {
  p_values <- numeric(num_bootstraps)
  
  for (i in 1:num_bootstraps) {
    # Bootstrap: Sample with replacement by patient ID
    boot_patient_ids <- sample(unique(data$USUBJID), size = length(unique(data$USUBJID)), replace = TRUE)
    boot_data <- data %>% filter(USUBJID %in% boot_patient_ids)
    
    # Fit the GLS model on the bootstrapped data
    gls_model <- gls(RSORRES ~ 1 + VISIT + I(VISIT^2) + TREAT, data = boot_data, 
                     correlation = corAR1(form = ~ 1 | USUBJID))
    summary_gls <- summary(gls_model)
    
    # Extract the p-value for the treatment effect
    coefficients <- summary_gls$tTable[, "Value"]
    standard_errors <- summary_gls$tTable[, "Std.Error"]
    z_scores <- coefficients / standard_errors
    p_values[i] <- 2 * pnorm(abs(z_scores["TREAT"]), lower.tail = FALSE)
  }
  
  return(p_values)
}

# Simulate the original data if not already available
set.seed(123)
# Assuming rho, sigma, N are already defined from previous work
# Simulate data here if necessary...

# Run bootstrap analysis
set.seed(123)
bootstrap_p_values <- bootstrap_analysis(simulated_data, num_bootstraps = 1000)
sig_values <- sum(bootstrap_p_values < 0.05) / length(bootstrap_p_values)

# Plot the distribution of p-values
ggplot(data.frame(PValue = bootstrap_p_values), aes(x = PValue)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of P-Values from Bootstrap", x = "P-Value", y = "Frequency")

print(sig_values)
```
Ask chris about the statistical power stuff - why do I need to perform t-test on residuals. What does t-test on residuals actually show? Surely I perform a t-test on the actual simulated scores at a chosen time-point - probably best to choose baseline and also last visit. We probably won't be able to distinguish between patients at baseline but potentially at the last visit.

```{r, warning=False}
# Assuming you've installed and loaded simr and lme4
library(lme4)
library(simr)

# Fit a model (example using lmer for a similar structure)
model <- lmer(RSORRES ~ VISIT + I(VISIT^2) + TREAT + (1 | USUBJID), data = simulated_data)

# Estimate power to detect a significant effect of treatment
results <- powerSim(model, fixed("TREAT"), nsim = 50)  # nsim is the number of simulations

# Check results
print(results)
```
